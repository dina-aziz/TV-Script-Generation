{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TV Script Generation\n",
    "\n",
    "In this project, you'll generate your own [Seinfeld](https://en.wikipedia.org/wiki/Seinfeld) TV scripts using RNNs.  You'll be using part of the [Seinfeld dataset](https://www.kaggle.com/thec03u5/seinfeld-chronicles#scripts.csv) of scripts from 9 seasons.  The Neural Network you'll build will generate a new ,\"fake\" TV script, based on patterns it recognizes in this training data.\n",
    "\n",
    "## Get the Data\n",
    "\n",
    "The data is already provided for you in `./data/Seinfeld_Scripts.txt` and you're encouraged to open that file and look at the text. \n",
    ">* As a first step, we'll load in this data and look at some samples. \n",
    "* Then, you'll be tasked with defining and training an RNN to generate a new script!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# load in data\n",
    "import helper\n",
    "data_dir = './data/Seinfeld_Scripts.txt'\n",
    "text = helper.load_data(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data\n",
    "Play around with `view_line_range` to view different parts of the data. This will give you a sense of the data you'll be working with. You can see, for example, that it is all lowercase text, and each new line of dialogue is separated by a newline character `\\n`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Stats\n",
      "Roughly the number of unique words: 46367\n",
      "Number of lines: 109233\n",
      "Average number of words in each line: 5.544240293684143\n",
      "\n",
      "The lines 0 to 10:\n",
      "jerry: do you know what this is all about? do you know, why were here? to be out, this is out...and out is one of the single most enjoyable experiences of life. people...did you ever hear people talking about we should go out? this is what theyre talking about...this whole thing, were all out now, no one is home. not one person here is home, were all out! there are people trying to find us, they dont know where we are. (on an imaginary phone) did you ring?, i cant find him. where did he go? he didnt tell me where he was going. he must have gone out. you wanna go out you get ready, you pick out the clothes, right? you take the shower, you get all ready, get the cash, get your friends, the car, the spot, the reservation...then youre standing around, what do you do? you go we gotta be getting back. once youre out, you wanna get back! you wanna go to sleep, you wanna get up, you wanna go out again tomorrow, right? where ever you are in life, its my feeling, youve gotta go. \n",
      "\n",
      "jerry: (pointing at georges shirt) see, to me, that button is in the worst possible spot. the second button literally makes or breaks the shirt, look at it. its too high! its in no-mans-land. you look like you live with your mother. \n",
      "\n",
      "george: are you through? \n",
      "\n",
      "jerry: you do of course try on, when you buy? \n",
      "\n",
      "george: yes, it was purple, i liked it, i dont actually recall considering the buttons. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "view_line_range = (0, 10)\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "print('Dataset Stats')\n",
    "print('Roughly the number of unique words: {}'.format(len({word: None for word in text.split()})))\n",
    "\n",
    "lines = text.split('\\n')\n",
    "print('Number of lines: {}'.format(len(lines)))\n",
    "word_count_line = [len(line.split()) for line in lines]\n",
    "print('Average number of words in each line: {}'.format(np.average(word_count_line)))\n",
    "\n",
    "print()\n",
    "print('The lines {} to {}:'.format(*view_line_range))\n",
    "print('\\n'.join(text.split('\\n')[view_line_range[0]:view_line_range[1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Implement Pre-processing Functions\n",
    "The first thing to do to any dataset is pre-processing.  Implement the following pre-processing functions below:\n",
    "- Lookup Table\n",
    "- Tokenize Punctuation\n",
    "\n",
    "### Lookup Table\n",
    "To create a word embedding, you first need to transform the words to ids.  In this function, create two dictionaries:\n",
    "- Dictionary to go from the words to an id, we'll call `vocab_to_int`\n",
    "- Dictionary to go from the id to word, we'll call `int_to_vocab`\n",
    "\n",
    "Return these dictionaries in the following **tuple** `(vocab_to_int, int_to_vocab)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "import problem_unittests as tests\n",
    "\n",
    "def create_lookup_tables(text):\n",
    "    \"\"\"\n",
    "    Create lookup tables for vocabulary\n",
    "    :param text: The text of tv scripts split into words\n",
    "    :return: A tuple of dicts (vocab_to_int, int_to_vocab)\n",
    "    \"\"\"\n",
    "     # TODO: Implement Function\n",
    "    \n",
    "    int_to_vocab = {ii: word for ii, word in enumerate(set(text))}\n",
    "    vocab_to_int = {word: ii for ii, word in int_to_vocab.items()}\n",
    "    # return tuple\n",
    "    return (vocab_to_int, int_to_vocab)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_create_lookup_tables(create_lookup_tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Punctuation\n",
    "We'll be splitting the script into a word array using spaces as delimiters.  However, punctuations like periods and exclamation marks can create multiple ids for the same word. For example, \"bye\" and \"bye!\" would generate two different word ids.\n",
    "\n",
    "Implement the function `token_lookup` to return a dict that will be used to tokenize symbols like \"!\" into \"||Exclamation_Mark||\".  Create a dictionary for the following symbols where the symbol is the key and value is the token:\n",
    "- Period ( **.** )\n",
    "- Comma ( **,** )\n",
    "- Quotation Mark ( **\"** )\n",
    "- Semicolon ( **;** )\n",
    "- Exclamation mark ( **!** )\n",
    "- Question mark ( **?** )\n",
    "- Left Parentheses ( **(** )\n",
    "- Right Parentheses ( **)** )\n",
    "- Dash ( **-** )\n",
    "- Return ( **\\n** )\n",
    "\n",
    "This dictionary will be used to tokenize the symbols and add the delimiter (space) around it.  This separates each symbols as its own word, making it easier for the neural network to predict the next word. Make sure you don't use a value that could be confused as a word; for example, instead of using the value \"dash\", try using something like \"||dash||\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def token_lookup():\n",
    "    \"\"\"\n",
    "    Generate a dict to turn punctuation into a token.\n",
    "    :return: Tokenized dictionary where the key is the punctuation and the value is the token\n",
    "    \"\"\"\n",
    "    dict = {\n",
    "        '.' : \"||Period||\",\n",
    "        ',' : \"||Comma||\",\n",
    "        '\"' : \"||Quotation_Mark||\",\n",
    "        ';' : \"||Semicolon||\",\n",
    "        '!' : \"||Exclamation_Mark||\",\n",
    "        '?' : \"||Question_Mark||\",\n",
    "        '(' : \"||Left_Parentheses||\",\n",
    "        ')' : \"||Right_Parentheses||\",\n",
    "        '-' : \"||Dash||\",\n",
    "        '\\n' : \"||Return||\" \n",
    "    }\n",
    "        \n",
    "    return dict\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_tokenize(token_lookup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process all the data and save it\n",
    "\n",
    "Running the code cell below will pre-process all the data and save it to file. You're encouraged to lok at the code for `preprocess_and_save_data` in the `helpers.py` file to see what it's doing in detail, but you do not need to change this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# pre-process training data\n",
    "helper.preprocess_and_save_data(data_dir, token_lookup, create_lookup_tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Point\n",
    "This is your first checkpoint. If you ever decide to come back to this notebook or have to restart the notebook, you can start from here. The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import helper\n",
    "import problem_unittests as tests\n",
    "\n",
    "int_text, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Neural Network\n",
    "In this section, you'll build the components necessary to build an RNN by implementing the RNN Module and forward and backpropagation functions.\n",
    "\n",
    "### Check Access to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import torch\n",
    "\n",
    "# Check for a GPU\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if not train_on_gpu:\n",
    "    print('No GPU found. Please use a GPU to train your neural network.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input\n",
    "Let's start with the preprocessed input data. We'll use [TensorDataset](http://pytorch.org/docs/master/data.html#torch.utils.data.TensorDataset) to provide a known format to our dataset; in combination with [DataLoader](http://pytorch.org/docs/master/data.html#torch.utils.data.DataLoader), it will handle batching, shuffling, and other dataset iteration functions.\n",
    "\n",
    "You can create data with TensorDataset by passing in feature and target tensors. Then create a DataLoader as usual.\n",
    "```\n",
    "data = TensorDataset(feature_tensors, target_tensors)\n",
    "data_loader = torch.utils.data.DataLoader(data, \n",
    "                                          batch_size=batch_size)\n",
    "```\n",
    "\n",
    "### Batching\n",
    "Implement the `batch_data` function to batch `words` data into chunks of size `batch_size` using the `TensorDataset` and `DataLoader` classes.\n",
    "\n",
    ">You can batch words using the DataLoader, but it will be up to you to create `feature_tensors` and `target_tensors` of the correct size and content for a given `sequence_length`.\n",
    "\n",
    "For example, say we have these as input:\n",
    "```\n",
    "words = [1, 2, 3, 4, 5, 6, 7]\n",
    "sequence_length = 4\n",
    "```\n",
    "\n",
    "Your first `feature_tensor` should contain the values:\n",
    "```\n",
    "[1, 2, 3, 4]\n",
    "```\n",
    "And the corresponding `target_tensor` should just be the next \"word\"/tokenized word value:\n",
    "```\n",
    "5\n",
    "```\n",
    "This should continue with the second `feature_tensor`, `target_tensor` being:\n",
    "```\n",
    "[2, 3, 4, 5]  # features\n",
    "6             # target\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "||period||\n",
      "||return||\n",
      "||return||\n",
      "kramer:\n",
      "hey\n",
      "||exclamation_mark||\n",
      "||return||\n",
      "||return||\n",
      "george:\n",
      "||left_parentheses||\n",
      "shouting\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "def batch_data(words, sequence_length, batch_size):\n",
    "    \"\"\"\n",
    "    Batch the neural network data using DataLoader\n",
    "    :param words: The word ids of the TV scripts\n",
    "    :param sequence_length: The sequence length of each batch\n",
    "    :param batch_size: The size of each batch; the number of sequences in a batch\n",
    "    :return: DataLoader with batched data\n",
    "    \"\"\"\n",
    "     # TODO: Implement function\n",
    "    feature_tensors=[]\n",
    "    target_tensors=[]\n",
    "    n_batches = len(words)// batch_size\n",
    "    words = words[:n_batches*batch_size]\n",
    "    for idx in range(len(words)-sequence_length):\n",
    "        feature_tensors.append(words[idx:idx+sequence_length])\n",
    "        target_tensors.append(words[idx+sequence_length])\n",
    "        \n",
    "    feature_tensors = torch.from_numpy(np.array(feature_tensors))\n",
    "    target_tensors = torch.from_numpy(np.array(target_tensors))\n",
    "    \n",
    "    data = TensorDataset(feature_tensors, target_tensors)\n",
    "    data_loader = torch.utils.data.DataLoader(data, batch_size = batch_size, shuffle =True)\n",
    "    # return a dataloader\n",
    "    return data_loader\n",
    "\n",
    "loader = batch_data(int_text,10, 32)\n",
    "data_iter = iter(loader)\n",
    "x, y = data_iter.next()\n",
    "for i in range(10):\n",
    "    print(int_to_vocab[int(x[0][i])])\n",
    "print(int_to_vocab[int(y[0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test your dataloader \n",
    "\n",
    "You'll have to modify this code to test a batching function, but it should look fairly similar.\n",
    "\n",
    "Below, we're generating some test text data and defining a dataloader using the function you defined, above. Then, we are getting some sample batch of inputs `sample_x` and targets `sample_y` from our dataloader.\n",
    "\n",
    "Your code should return something like the following (likely in a different order, if you shuffled your data):\n",
    "\n",
    "```\n",
    "torch.Size([10, 5])\n",
    "tensor([[ 28,  29,  30,  31,  32],\n",
    "        [ 21,  22,  23,  24,  25],\n",
    "        [ 17,  18,  19,  20,  21],\n",
    "        [ 34,  35,  36,  37,  38],\n",
    "        [ 11,  12,  13,  14,  15],\n",
    "        [ 23,  24,  25,  26,  27],\n",
    "        [  6,   7,   8,   9,  10],\n",
    "        [ 38,  39,  40,  41,  42],\n",
    "        [ 25,  26,  27,  28,  29],\n",
    "        [  7,   8,   9,  10,  11]])\n",
    "\n",
    "torch.Size([10])\n",
    "tensor([ 33,  26,  22,  39,  16,  28,  11,  43,  30,  12])\n",
    "```\n",
    "\n",
    "### Sizes\n",
    "Your sample_x should be of size `(batch_size, sequence_length)` or (10, 5) in this case and sample_y should just have one dimension: batch_size (10). \n",
    "\n",
    "### Values\n",
    "\n",
    "You should also notice that the targets, sample_y, are the *next* value in the ordered test_text data. So, for an input sequence `[ 28,  29,  30,  31,  32]` that ends with the value `32`, the corresponding output should be `33`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 5])\n",
      "tensor([[ 11,  12,  13,  14,  15],\n",
      "        [ 36,  37,  38,  39,  40],\n",
      "        [ 40,  41,  42,  43,  44],\n",
      "        [ 12,  13,  14,  15,  16],\n",
      "        [  7,   8,   9,  10,  11],\n",
      "        [  3,   4,   5,   6,   7],\n",
      "        [ 42,  43,  44,  45,  46],\n",
      "        [ 38,  39,  40,  41,  42],\n",
      "        [  4,   5,   6,   7,   8],\n",
      "        [ 25,  26,  27,  28,  29]])\n",
      "\n",
      "torch.Size([10])\n",
      "tensor([ 16,  41,  45,  17,  12,   8,  47,  43,   9,  30])\n"
     ]
    }
   ],
   "source": [
    "# test dataloader\n",
    "\n",
    "test_text = range(50)\n",
    "t_loader = batch_data(test_text, sequence_length=5, batch_size=10)\n",
    "\n",
    "data_iter = iter(t_loader)\n",
    "sample_x, sample_y = data_iter.next()\n",
    "\n",
    "print(sample_x.shape)\n",
    "print(sample_x)\n",
    "print()\n",
    "print(sample_y.shape)\n",
    "print(sample_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Build the Neural Network\n",
    "Implement an RNN using PyTorch's [Module class](http://pytorch.org/docs/master/nn.html#torch.nn.Module). You may choose to use a GRU or an LSTM. To complete the RNN, you'll have to implement the following functions for the class:\n",
    " - `__init__` - The initialize function. \n",
    " - `init_hidden` - The initialization function for an LSTM/GRU hidden state\n",
    " - `forward` - Forward propagation function.\n",
    " \n",
    "The initialize function should create the layers of the neural network and save them to the class. The forward propagation function will use these layers to run forward propagation and generate an output and a hidden state.\n",
    "\n",
    "**The output of this model should be the *last* batch of word scores** after a complete sequence has been processed. That is, for each input sequence of words, we only want to output the word scores for a single, most likely, next word.\n",
    "\n",
    "### Hints\n",
    "\n",
    "1. Make sure to stack the outputs of the lstm to pass to your fully-connected layer, you can do this with `lstm_output = lstm_output.contiguous().view(-1, self.hidden_dim)`\n",
    "2. You can get the last batch of word scores by shaping the output of the final, fully-connected layer like so:\n",
    "\n",
    "```\n",
    "# reshape into (batch_size, seq_length, output_size)\n",
    "output = output.view(batch_size, -1, self.output_size)\n",
    "# get last batch\n",
    "out = output[:, -1]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the PyTorch RNN Module\n",
    "        :param vocab_size: The number of input dimensions of the neural network (the size of the vocabulary)\n",
    "        :param output_size: The number of output dimensions of the neural network\n",
    "        :param embedding_dim: The size of embeddings, should you choose to use them        \n",
    "        :param hidden_dim: The size of the hidden layer outputs\n",
    "        :param dropout: dropout to add in between LSTM/GRU layers\n",
    "        \"\"\"\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        # TODO: Implement function\n",
    "        \n",
    "        # set class variables\n",
    "        self.vocab_size = vocab_size\n",
    "        self.output_size = output_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = hidden_dim\n",
    "        self.drop_prob = dropout\n",
    "        \n",
    "        # define model layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size,embedding_dim)\n",
    "        self.dropout = nn.Dropout(p=self.drop_prob)\n",
    "        self.lstm= nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        \n",
    "    def forward(self, nn_input, hidden):\n",
    "        \"\"\"\n",
    "        Forward propagation of the neural network\n",
    "        :param nn_input: The input to the neural network\n",
    "        :param hidden: The hidden state        \n",
    "        :return: Two Tensors, the output of the neural network and the latest hidden state\n",
    "        \"\"\"\n",
    "        # TODO: Implement function  \n",
    "        x_embed = self.embedding(nn_input)\n",
    "        #x= self.dropout(x_embed)\n",
    "        lstm_out, hidden = self.lstm(x_embed, hidden)\n",
    "        out= self.dropout(lstm_out)\n",
    "        output=out.contiguous()\n",
    "        output=output.view(-1,  self.n_hidden)\n",
    "        output= self.fc(out)\n",
    "        batch_size = nn_input.shape[0]\n",
    "        output = output.view(batch_size, -1, self.output_size)\n",
    "        out = output[:, -1]\n",
    "        # return one batch of output word scores and the hidden state\n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        '''\n",
    "        Initialize the hidden state of an LSTM/GRU\n",
    "        :param batch_size: The batch_size of the hidden state\n",
    "        :return: hidden state of dims (n_layers, batch_size, hidden_dim)\n",
    "        '''\n",
    "        # Implement function\n",
    "        \n",
    "        # initialize hidden state with zero weights, and move to GPU if available\n",
    "        weight = next(self.parameters()).data\n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "        \n",
    "        return hidden\n",
    "        \n",
    "    \n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_rnn(RNN, train_on_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define forward and backpropagation\n",
    "\n",
    "Use the RNN class you implemented to apply forward and back propagation. This function will be called, iteratively, in the training loop as follows:\n",
    "```\n",
    "loss = forward_back_prop(decoder, decoder_optimizer, criterion, inp, target)\n",
    "```\n",
    "\n",
    "And it should return the average loss over a batch and the hidden state returned by a call to `RNN(inp, hidden)`. Recall that you can get this loss by computing it, as usual, and calling `loss.item()`.\n",
    "\n",
    "**If a GPU is available, you should move your data to that GPU device, here.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def forward_back_prop(rnn, optimizer, criterion, inp, target, hidden):\n",
    "    \"\"\"\n",
    "    Forward and backward propagation on the neural network\n",
    "    :param decoder: The PyTorch Module that holds the neural network\n",
    "    :param decoder_optimizer: The PyTorch optimizer for the neural network\n",
    "    :param criterion: The PyTorch loss function\n",
    "    :param inp: A batch of input to the neural network\n",
    "    :param target: The target output for the batch of input\n",
    "    :return: The loss and the latest hidden state Tensor\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Implement Function\n",
    "    \n",
    "   \n",
    "    # move data to GPU, if available\n",
    "    if(train_on_gpu):\n",
    "        inp, target = inp.cuda(), target.cuda()\n",
    "\n",
    "    h = tuple([each.data for each in hidden])\n",
    "    rnn.zero_grad()\n",
    "    output, h = rnn(inp, h)\n",
    "    \n",
    "    # perform backpropagation and optimization\n",
    "    target= target.contiguous()  \n",
    "    loss = criterion(output, target)\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(rnn.parameters(), 10)\n",
    "    optimizer.step()\n",
    "   \n",
    "\n",
    "    # return the loss over a batch and the hidden state produced by our model\n",
    "    return loss.item(), h\n",
    "\n",
    "# Note that these tests aren't completely extensive.\n",
    "# they are here to act as general checks on the expected outputs of your functions\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_forward_back_prop(RNN, forward_back_prop, train_on_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Training\n",
    "\n",
    "With the structure of the network complete and data ready to be fed in the neural network, it's time to train it.\n",
    "\n",
    "### Train Loop\n",
    "\n",
    "The training loop is implemented for you in the `train_decoder` function. This function will train the network over all the batches for the number of epochs given. The model progress will be shown every number of batches. This number is set with the `show_every_n_batches` parameter. You'll set this parameter along with other parameters in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "\n",
    "def train_rnn(rnn, batch_size, optimizer, criterion, n_epochs, show_every_n_batches=100):\n",
    "    batch_losses = []\n",
    "    least_loss = np.inf\n",
    "    rnn.train()\n",
    "\n",
    "    print(\"Training for %d epoch(s)...\" % n_epochs)\n",
    "    for epoch_i in range(1, n_epochs + 1):\n",
    "        \n",
    "        # initialize hidden state\n",
    "        hidden = rnn.init_hidden(batch_size)\n",
    "        \n",
    "        for batch_i, (inputs, labels) in enumerate(train_loader, 1):\n",
    "            \n",
    "            # make sure you iterate over completely full batches, only\n",
    "            n_batches = len(train_loader.dataset)//batch_size\n",
    "            if(batch_i > n_batches):\n",
    "                break\n",
    "            \n",
    "            # forward, back prop\n",
    "            loss, hidden = forward_back_prop(rnn, optimizer, criterion, inputs, labels, hidden)          \n",
    "            # record loss\n",
    "            batch_losses.append(loss)\n",
    "           \n",
    "            # printing loss stats\n",
    "            if batch_i % show_every_n_batches == 0:\n",
    "                avg_loss = np.average(batch_losses)\n",
    "                print('\\nEpoch: {:>4}/{:<4}  Loss: {}    Least Loss: {}'.format(epoch_i, n_epochs, avg_loss, least_loss))\n",
    "                if  avg_loss < least_loss:\n",
    "                    helper.save_model('./save/trained_rnn', rnn)\n",
    "                    print('      Model Saved ..')\n",
    "                    least_loss =  avg_loss\n",
    "                batch_losses = []\n",
    "\n",
    "    # returns a trained rnn\n",
    "    return rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "\n",
    "Set and train the neural network with the following parameters:\n",
    "- Set `sequence_length` to the length of a sequence.\n",
    "- Set `batch_size` to the batch size.\n",
    "- Set `num_epochs` to the number of epochs to train for.\n",
    "- Set `learning_rate` to the learning rate for an Adam optimizer.\n",
    "- Set `vocab_size` to the number of uniqe tokens in our vocabulary.\n",
    "- Set `output_size` to the desired size of the output.\n",
    "- Set `embedding_dim` to the embedding dimension; smaller than the vocab_size.\n",
    "- Set `hidden_dim` to the hidden dimension of your RNN.\n",
    "- Set `n_layers` to the number of layers/cells in your RNN.\n",
    "- Set `show_every_n_batches` to the number of batches at which the neural network should print progress.\n",
    "\n",
    "If the network isn't getting the desired results, tweak these parameters and/or the layers in the `RNN` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data params\n",
    "# Sequence Length\n",
    "sequence_length =7  # of words in a sequence\n",
    "# Batch Size\n",
    "batch_size = 32\n",
    "\n",
    "# data loader - do not change\n",
    "train_loader = batch_data(int_text, sequence_length, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "# Number of Epochs\n",
    "num_epochs = 10\n",
    "# Learning Rate\n",
    "learning_rate = 0.0005\n",
    "\n",
    "# Model parameters\n",
    "# Vocab size\n",
    "vocab_size = len(vocab_to_int)\n",
    "# Output size\n",
    "output_size = vocab_size\n",
    "# Embedding Dimension\n",
    "embedding_dim =300\n",
    "# Hidden Dimension\n",
    "hidden_dim =  300\n",
    "# Number of RNN Layers\n",
    "n_layers = 2\n",
    "\n",
    "# Show stats for every n number of batches\n",
    "show_every_n_batches = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train\n",
    "In the next cell, you'll train the neural network on the pre-processed data.  If you have a hard time getting a good loss, you may consider changing your hyperparameters. In general, you may get better results with larger hidden and n_layer dimensions, but larger models take a longer time to train. \n",
    "> **You should aim for a loss less than 3.5.** \n",
    "\n",
    "You should also experiment with different sequence lengths, which determine the size of the long range dependencies that a model can learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "\n",
    "import workspace_utils\n",
    "from workspace_utils import active_session\n",
    "\n",
    "# create model and move to gpu if available\n",
    "rnn = RNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.5)\n",
    "if train_on_gpu:\n",
    "    rnn.cuda()\n",
    "\n",
    "# defining loss and optimization functions for training\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 10 epoch(s)...\n",
      "\n",
      "Epoch:    1/10    Loss: 5.907459856510163    Least Loss: inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/torch/serialization.py:193: UserWarning: Couldn't retrieve source code for container of type RNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Model Saved ..\n",
      "\n",
      "Epoch:    1/10    Loss: 5.241622133731842    Least Loss: 5.907459856510163\n",
      "      Model Saved ..\n",
      "\n",
      "Epoch:    1/10    Loss: 5.1570989789962765    Least Loss: 5.241622133731842\n",
      "      Model Saved ..\n",
      "\n",
      "Epoch:    1/10    Loss: 4.9949976263046265    Least Loss: 5.1570989789962765\n",
      "      Model Saved ..\n",
      "\n",
      "Epoch:    1/10    Loss: 4.919566945075989    Least Loss: 4.9949976263046265\n",
      "      Model Saved ..\n",
      "\n",
      "Epoch:    1/10    Loss: 4.85281715965271    Least Loss: 4.919566945075989\n",
      "      Model Saved ..\n",
      "\n",
      "Epoch:    1/10    Loss: 4.795255446434021    Least Loss: 4.85281715965271\n",
      "      Model Saved ..\n",
      "\n",
      "Epoch:    1/10    Loss: 4.665177387237549    Least Loss: 4.795255446434021\n",
      "      Model Saved ..\n",
      "\n",
      "Epoch:    1/10    Loss: 4.7175405716896055    Least Loss: 4.665177387237549\n",
      "\n",
      "Epoch:    1/10    Loss: 4.681144971370697    Least Loss: 4.665177387237549\n",
      "\n",
      "Epoch:    1/10    Loss: 4.679413398265838    Least Loss: 4.665177387237549\n",
      "\n",
      "Epoch:    1/10    Loss: 4.630890210151672    Least Loss: 4.665177387237549\n",
      "      Model Saved ..\n",
      "\n",
      "Epoch:    1/10    Loss: 4.644197685718536    Least Loss: 4.630890210151672\n",
      "\n",
      "Epoch:    1/10    Loss: 4.601523571491241    Least Loss: 4.630890210151672\n",
      "      Model Saved ..\n",
      "\n",
      "Epoch:    1/10    Loss: 4.581237937450409    Least Loss: 4.601523571491241\n",
      "      Model Saved ..\n",
      "\n",
      "Epoch:    1/10    Loss: 4.583550642490387    Least Loss: 4.581237937450409\n",
      "\n",
      "Epoch:    1/10    Loss: 4.48296317577362    Least Loss: 4.581237937450409\n",
      "      Model Saved ..\n",
      "\n",
      "Epoch:    1/10    Loss: 4.452785509109497    Least Loss: 4.48296317577362\n",
      "      Model Saved ..\n",
      "\n",
      "Epoch:    1/10    Loss: 4.568292897224426    Least Loss: 4.452785509109497\n",
      "\n",
      "Epoch:    1/10    Loss: 4.5021611886024475    Least Loss: 4.452785509109497\n",
      "\n",
      "Epoch:    1/10    Loss: 4.481605321407318    Least Loss: 4.452785509109497\n",
      "\n",
      "Epoch:    1/10    Loss: 4.506941857337952    Least Loss: 4.452785509109497\n",
      "\n",
      "Epoch:    1/10    Loss: 4.472622176170349    Least Loss: 4.452785509109497\n",
      "\n",
      "Epoch:    1/10    Loss: 4.477962113857269    Least Loss: 4.452785509109497\n",
      "\n",
      "Epoch:    1/10    Loss: 4.4690945239067075    Least Loss: 4.452785509109497\n",
      "\n",
      "Epoch:    1/10    Loss: 4.427947051525116    Least Loss: 4.452785509109497\n",
      "      Model Saved ..\n",
      "\n",
      "Epoch:    1/10    Loss: 4.427349191188812    Least Loss: 4.427947051525116\n",
      "      Model Saved ..\n",
      "\n",
      "Epoch:    1/10    Loss: 4.441472480297088    Least Loss: 4.427349191188812\n",
      "\n",
      "Epoch:    1/10    Loss: 4.382725916385651    Least Loss: 4.427349191188812\n",
      "      Model Saved ..\n",
      "\n",
      "Epoch:    1/10    Loss: 4.420769117355347    Least Loss: 4.382725916385651\n",
      "\n",
      "Epoch:    1/10    Loss: 4.4489217114448545    Least Loss: 4.382725916385651\n",
      "\n",
      "Epoch:    1/10    Loss: 4.410536070823669    Least Loss: 4.382725916385651\n",
      "\n",
      "Epoch:    1/10    Loss: 4.4460034799575805    Least Loss: 4.382725916385651\n",
      "\n",
      "Epoch:    1/10    Loss: 4.411748754024505    Least Loss: 4.382725916385651\n",
      "\n",
      "Epoch:    1/10    Loss: 4.406443130970001    Least Loss: 4.382725916385651\n",
      "\n",
      "Epoch:    1/10    Loss: 4.338810949325562    Least Loss: 4.382725916385651\n",
      "      Model Saved ..\n",
      "\n",
      "Epoch:    1/10    Loss: 4.368956088066101    Least Loss: 4.338810949325562\n",
      "\n",
      "Epoch:    1/10    Loss: 4.344053831100464    Least Loss: 4.338810949325562\n",
      "\n",
      "Epoch:    1/10    Loss: 4.36298557472229    Least Loss: 4.338810949325562\n",
      "\n",
      "Epoch:    1/10    Loss: 4.417942876815796    Least Loss: 4.338810949325562\n",
      "\n",
      "Epoch:    1/10    Loss: 4.375529882907867    Least Loss: 4.338810949325562\n",
      "\n",
      "Epoch:    1/10    Loss: 4.381275224685669    Least Loss: 4.338810949325562\n",
      "\n",
      "Epoch:    1/10    Loss: 4.347345617771149    Least Loss: 4.338810949325562\n",
      "\n",
      "Epoch:    1/10    Loss: 4.360114883899689    Least Loss: 4.338810949325562\n",
      "\n",
      "Epoch:    1/10    Loss: 4.408947266578674    Least Loss: 4.338810949325562\n",
      "\n",
      "Epoch:    1/10    Loss: 4.376450763702392    Least Loss: 4.338810949325562\n",
      "\n",
      "Epoch:    1/10    Loss: 4.387784944534301    Least Loss: 4.338810949325562\n",
      "\n",
      "Epoch:    1/10    Loss: 4.360506106376648    Least Loss: 4.338810949325562\n",
      "\n",
      "Epoch:    1/10    Loss: 4.3453577294349675    Least Loss: 4.338810949325562\n",
      "\n",
      "Epoch:    1/10    Loss: 4.329788533210754    Least Loss: 4.338810949325562\n",
      "      Model Saved ..\n",
      "\n",
      "Epoch:    1/10    Loss: 4.316005686283112    Least Loss: 4.329788533210754\n",
      "      Model Saved ..\n",
      "\n",
      "Epoch:    1/10    Loss: 4.333450723171234    Least Loss: 4.316005686283112\n",
      "\n",
      "Epoch:    1/10    Loss: 4.321792492866516    Least Loss: 4.316005686283112\n",
      "\n",
      "Epoch:    1/10    Loss: 4.255175036430359    Least Loss: 4.316005686283112\n",
      "      Model Saved ..\n",
      "\n",
      "Epoch:    1/10    Loss: 4.320833295822143    Least Loss: 4.255175036430359\n",
      "\n",
      "Epoch:    2/10    Loss: 4.221206097129813    Least Loss: 4.255175036430359\n",
      "      Model Saved ..\n",
      "\n",
      "Epoch:    2/10    Loss: 4.21957672405243    Least Loss: 4.221206097129813\n",
      "      Model Saved ..\n",
      "\n",
      "Epoch:    2/10    Loss: 4.200994379997254    Least Loss: 4.21957672405243\n",
      "      Model Saved ..\n",
      "\n",
      "Epoch:    2/10    Loss: 4.189904616355896    Least Loss: 4.200994379997254\n",
      "      Model Saved ..\n",
      "\n",
      "Epoch:    2/10    Loss: 4.221727045059204    Least Loss: 4.189904616355896\n",
      "\n",
      "Epoch:    2/10    Loss: 4.195049339771271    Least Loss: 4.189904616355896\n",
      "\n",
      "Epoch:    2/10    Loss: 4.208955990314483    Least Loss: 4.189904616355896\n",
      "\n",
      "Epoch:    2/10    Loss: 4.20631565618515    Least Loss: 4.189904616355896\n",
      "\n",
      "Epoch:    2/10    Loss: 4.190909840583801    Least Loss: 4.189904616355896\n",
      "\n",
      "Epoch:    2/10    Loss: 4.24142974948883    Least Loss: 4.189904616355896\n",
      "\n",
      "Epoch:    2/10    Loss: 4.181463073730469    Least Loss: 4.189904616355896\n",
      "      Model Saved ..\n",
      "\n",
      "Epoch:    2/10    Loss: 4.256778377056122    Least Loss: 4.181463073730469\n",
      "\n",
      "Epoch:    2/10    Loss: 4.2413651707172395    Least Loss: 4.181463073730469\n",
      "\n",
      "Epoch:    2/10    Loss: 4.190766030788422    Least Loss: 4.181463073730469\n",
      "\n",
      "Epoch:    2/10    Loss: 4.227902336597443    Least Loss: 4.181463073730469\n",
      "\n",
      "Epoch:    2/10    Loss: 4.192074592113495    Least Loss: 4.181463073730469\n",
      "\n",
      "Epoch:    2/10    Loss: 4.173959046363831    Least Loss: 4.181463073730469\n",
      "      Model Saved ..\n",
      "\n",
      "Epoch:    2/10    Loss: 4.19393466758728    Least Loss: 4.173959046363831\n",
      "\n",
      "Epoch:    2/10    Loss: 4.2069280562400815    Least Loss: 4.173959046363831\n",
      "\n",
      "Epoch:    2/10    Loss: 4.193425266265869    Least Loss: 4.173959046363831\n",
      "\n",
      "Epoch:    2/10    Loss: 4.2230411167144775    Least Loss: 4.173959046363831\n",
      "\n",
      "Epoch:    2/10    Loss: 4.190362743854522    Least Loss: 4.173959046363831\n",
      "\n",
      "Epoch:    2/10    Loss: 4.153346856117248    Least Loss: 4.173959046363831\n",
      "      Model Saved ..\n",
      "\n",
      "Epoch:    2/10    Loss: 4.172956593990326    Least Loss: 4.153346856117248\n",
      "\n",
      "Epoch:    2/10    Loss: 4.167251104354858    Least Loss: 4.153346856117248\n",
      "\n",
      "Epoch:    2/10    Loss: 4.20177943944931    Least Loss: 4.153346856117248\n",
      "\n",
      "Epoch:    2/10    Loss: 4.19866593837738    Least Loss: 4.153346856117248\n",
      "\n",
      "Epoch:    2/10    Loss: 4.243860554218292    Least Loss: 4.153346856117248\n",
      "\n",
      "Epoch:    2/10    Loss: 4.205645991325379    Least Loss: 4.153346856117248\n",
      "\n",
      "Epoch:    2/10    Loss: 4.192987157821655    Least Loss: 4.153346856117248\n",
      "\n",
      "Epoch:    2/10    Loss: 4.244212080001831    Least Loss: 4.153346856117248\n",
      "\n",
      "Epoch:    2/10    Loss: 4.256680500984192    Least Loss: 4.153346856117248\n",
      "\n",
      "Epoch:    2/10    Loss: 4.236254444599152    Least Loss: 4.153346856117248\n",
      "\n",
      "Epoch:    2/10    Loss: 4.218809340953827    Least Loss: 4.153346856117248\n",
      "\n",
      "Epoch:    2/10    Loss: 4.242197762966156    Least Loss: 4.153346856117248\n",
      "\n",
      "Epoch:    2/10    Loss: 4.194969522953033    Least Loss: 4.153346856117248\n",
      "\n",
      "Epoch:    2/10    Loss: 4.236491221904755    Least Loss: 4.153346856117248\n",
      "\n",
      "Epoch:    2/10    Loss: 4.180745293140411    Least Loss: 4.153346856117248\n",
      "\n",
      "Epoch:    2/10    Loss: 4.229086568832398    Least Loss: 4.153346856117248\n",
      "\n",
      "Epoch:    2/10    Loss: 4.171811512470246    Least Loss: 4.153346856117248\n",
      "\n",
      "Epoch:    2/10    Loss: 4.174245507717132    Least Loss: 4.153346856117248\n",
      "\n",
      "Epoch:    2/10    Loss: 4.174831223011017    Least Loss: 4.153346856117248\n",
      "\n",
      "Epoch:    2/10    Loss: 4.220130505084992    Least Loss: 4.153346856117248\n",
      "\n",
      "Epoch:    2/10    Loss: 4.151005166530609    Least Loss: 4.153346856117248\n",
      "      Model Saved ..\n",
      "\n",
      "Epoch:    2/10    Loss: 4.173754444599152    Least Loss: 4.151005166530609\n",
      "\n",
      "Epoch:    2/10    Loss: 4.208875480175018    Least Loss: 4.151005166530609\n",
      "\n",
      "Epoch:    2/10    Loss: 4.1781842842102055    Least Loss: 4.151005166530609\n",
      "\n",
      "Epoch:    2/10    Loss: 4.203076768875122    Least Loss: 4.151005166530609\n",
      "\n",
      "Epoch:    2/10    Loss: 4.28227419424057    Least Loss: 4.151005166530609\n",
      "\n",
      "Epoch:    2/10    Loss: 4.218144826412201    Least Loss: 4.151005166530609\n",
      "\n",
      "Epoch:    2/10    Loss: 4.165216749191284    Least Loss: 4.151005166530609\n",
      "\n",
      "Epoch:    2/10    Loss: 4.248116060495376    Least Loss: 4.151005166530609\n",
      "\n",
      "Epoch:    2/10    Loss: 4.178206596851349    Least Loss: 4.151005166530609\n",
      "\n",
      "Epoch:    2/10    Loss: 4.200365434169769    Least Loss: 4.151005166530609\n",
      "\n",
      "Epoch:    2/10    Loss: 4.235324423789978    Least Loss: 4.151005166530609\n",
      "\n",
      "Epoch:    3/10    Loss: 4.119304715975389    Least Loss: 4.151005166530609\n",
      "      Model Saved ..\n",
      "\n",
      "Epoch:    3/10    Loss: 4.085065353393555    Least Loss: 4.119304715975389\n",
      "      Model Saved ..\n",
      "\n",
      "Epoch:    3/10    Loss: 4.078194474697113    Least Loss: 4.085065353393555\n",
      "      Model Saved ..\n",
      "\n",
      "Epoch:    3/10    Loss: 4.045189301967621    Least Loss: 4.078194474697113\n",
      "      Model Saved ..\n",
      "\n",
      "Epoch:    3/10    Loss: 4.092655948638916    Least Loss: 4.045189301967621\n",
      "\n",
      "Epoch:    3/10    Loss: 4.022818460464477    Least Loss: 4.045189301967621\n",
      "      Model Saved ..\n",
      "\n",
      "Epoch:    3/10    Loss: 4.140097511291504    Least Loss: 4.022818460464477\n",
      "\n",
      "Epoch:    3/10    Loss: 4.073553722858429    Least Loss: 4.022818460464477\n",
      "\n",
      "Epoch:    3/10    Loss: 4.057927296161652    Least Loss: 4.022818460464477\n",
      "\n",
      "Epoch:    3/10    Loss: 4.032040787696839    Least Loss: 4.022818460464477\n",
      "\n",
      "Epoch:    3/10    Loss: 4.103150111675262    Least Loss: 4.022818460464477\n",
      "\n",
      "Epoch:    3/10    Loss: 4.059533666610718    Least Loss: 4.022818460464477\n",
      "\n",
      "Epoch:    3/10    Loss: 4.111769525527954    Least Loss: 4.022818460464477\n",
      "\n",
      "Epoch:    3/10    Loss: 4.110961104869842    Least Loss: 4.022818460464477\n",
      "\n",
      "Epoch:    3/10    Loss: 4.078195465087891    Least Loss: 4.022818460464477\n",
      "\n",
      "Epoch:    3/10    Loss: 4.1006987414360045    Least Loss: 4.022818460464477\n",
      "\n",
      "Epoch:    3/10    Loss: 4.135823272228241    Least Loss: 4.022818460464477\n",
      "\n",
      "Epoch:    3/10    Loss: 4.104781867265701    Least Loss: 4.022818460464477\n",
      "\n",
      "Epoch:    3/10    Loss: 4.101225598812103    Least Loss: 4.022818460464477\n",
      "\n",
      "Epoch:    3/10    Loss: 4.133238605976104    Least Loss: 4.022818460464477\n",
      "\n",
      "Epoch:    3/10    Loss: 4.070918828487396    Least Loss: 4.022818460464477\n",
      "\n",
      "Epoch:    3/10    Loss: 4.05647441148758    Least Loss: 4.022818460464477\n",
      "\n",
      "Epoch:    3/10    Loss: 4.123242852687835    Least Loss: 4.022818460464477\n",
      "\n",
      "Epoch:    3/10    Loss: 4.111412465572357    Least Loss: 4.022818460464477\n",
      "\n",
      "Epoch:    3/10    Loss: 4.121847592830658    Least Loss: 4.022818460464477\n",
      "\n",
      "Epoch:    3/10    Loss: 4.099778685092926    Least Loss: 4.022818460464477\n",
      "\n",
      "Epoch:    3/10    Loss: 4.105427536725998    Least Loss: 4.022818460464477\n",
      "\n",
      "Epoch:    3/10    Loss: 4.1302288374900815    Least Loss: 4.022818460464477\n",
      "\n",
      "Epoch:    3/10    Loss: 4.088887597084045    Least Loss: 4.022818460464477\n",
      "\n",
      "Epoch:    3/10    Loss: 4.1308046894073485    Least Loss: 4.022818460464477\n",
      "\n",
      "Epoch:    3/10    Loss: 4.143323181152343    Least Loss: 4.022818460464477\n",
      "\n",
      "Epoch:    3/10    Loss: 4.166357458591461    Least Loss: 4.022818460464477\n",
      "\n",
      "Epoch:    3/10    Loss: 4.085377273321152    Least Loss: 4.022818460464477\n",
      "\n",
      "Epoch:    3/10    Loss: 4.109715916156769    Least Loss: 4.022818460464477\n",
      "\n",
      "Epoch:    3/10    Loss: 4.131319673061371    Least Loss: 4.022818460464477\n",
      "\n",
      "Epoch:    3/10    Loss: 4.152839147090912    Least Loss: 4.022818460464477\n",
      "\n",
      "Epoch:    3/10    Loss: 4.141069382190705    Least Loss: 4.022818460464477\n",
      "\n",
      "Epoch:    3/10    Loss: 4.174979680538177    Least Loss: 4.022818460464477\n",
      "\n",
      "Epoch:    3/10    Loss: 4.092977801322937    Least Loss: 4.022818460464477\n",
      "\n",
      "Epoch:    3/10    Loss: 4.111232988834381    Least Loss: 4.022818460464477\n",
      "\n",
      "Epoch:    3/10    Loss: 4.140839557170868    Least Loss: 4.022818460464477\n",
      "\n",
      "Epoch:    3/10    Loss: 4.123918840885162    Least Loss: 4.022818460464477\n",
      "\n",
      "Epoch:    3/10    Loss: 4.139057654857636    Least Loss: 4.022818460464477\n",
      "\n",
      "Epoch:    3/10    Loss: 4.142917295455932    Least Loss: 4.022818460464477\n",
      "\n",
      "Epoch:    3/10    Loss: 4.137466789722443    Least Loss: 4.022818460464477\n",
      "\n",
      "Epoch:    3/10    Loss: 4.153498374938965    Least Loss: 4.022818460464477\n",
      "\n",
      "Epoch:    3/10    Loss: 4.078361857891083    Least Loss: 4.022818460464477\n",
      "\n",
      "Epoch:    3/10    Loss: 4.086988429069519    Least Loss: 4.022818460464477\n",
      "\n",
      "Epoch:    3/10    Loss: 4.166745080471038    Least Loss: 4.022818460464477\n",
      "\n",
      "Epoch:    3/10    Loss: 4.129343005180359    Least Loss: 4.022818460464477\n",
      "\n",
      "Epoch:    3/10    Loss: 4.137707174777985    Least Loss: 4.022818460464477\n",
      "\n",
      "Epoch:    3/10    Loss: 4.176636889457702    Least Loss: 4.022818460464477\n",
      "\n",
      "Epoch:    3/10    Loss: 4.15900128364563    Least Loss: 4.022818460464477\n",
      "\n",
      "Epoch:    3/10    Loss: 4.140795122146606    Least Loss: 4.022818460464477\n",
      "\n",
      "Epoch:    3/10    Loss: 4.136802456378937    Least Loss: 4.022818460464477\n",
      "\n",
      "Epoch:    4/10    Loss: 4.061824498453869    Least Loss: 4.022818460464477\n",
      "\n",
      "Epoch:    4/10    Loss: 3.966062592983246    Least Loss: 4.022818460464477\n",
      "      Model Saved ..\n",
      "\n",
      "Epoch:    4/10    Loss: 3.975309168815613    Least Loss: 3.966062592983246\n",
      "\n",
      "Epoch:    4/10    Loss: 3.9892035298347475    Least Loss: 3.966062592983246\n",
      "\n",
      "Epoch:    4/10    Loss: 3.98957385635376    Least Loss: 3.966062592983246\n",
      "\n",
      "Epoch:    4/10    Loss: 4.017167037963867    Least Loss: 3.966062592983246\n",
      "\n",
      "Epoch:    4/10    Loss: 3.966022877693176    Least Loss: 3.966062592983246\n",
      "      Model Saved ..\n",
      "\n",
      "Epoch:    4/10    Loss: 4.029614848613739    Least Loss: 3.966022877693176\n",
      "\n",
      "Epoch:    4/10    Loss: 3.9789378571510317    Least Loss: 3.966022877693176\n",
      "\n",
      "Epoch:    4/10    Loss: 4.048694522380829    Least Loss: 3.966022877693176\n",
      "\n",
      "Epoch:    4/10    Loss: 3.996197902679443    Least Loss: 3.966022877693176\n",
      "\n",
      "Epoch:    4/10    Loss: 3.988080502510071    Least Loss: 3.966022877693176\n",
      "\n",
      "Epoch:    4/10    Loss: 4.072743691921234    Least Loss: 3.966022877693176\n",
      "\n",
      "Epoch:    4/10    Loss: 4.094666207313538    Least Loss: 3.966022877693176\n",
      "\n",
      "Epoch:    4/10    Loss: 4.023341951370239    Least Loss: 3.966022877693176\n",
      "\n",
      "Epoch:    4/10    Loss: 4.070245140075683    Least Loss: 3.966022877693176\n",
      "\n",
      "Epoch:    4/10    Loss: 4.011813522338867    Least Loss: 3.966022877693176\n",
      "\n",
      "Epoch:    4/10    Loss: 4.022078557014465    Least Loss: 3.966022877693176\n",
      "\n",
      "Epoch:    4/10    Loss: 4.0298539452552795    Least Loss: 3.966022877693176\n",
      "\n",
      "Epoch:    4/10    Loss: 4.033828943252564    Least Loss: 3.966022877693176\n",
      "\n",
      "Epoch:    4/10    Loss: 4.071583396434784    Least Loss: 3.966022877693176\n",
      "\n",
      "Epoch:    4/10    Loss: 4.004737565994263    Least Loss: 3.966022877693176\n",
      "\n",
      "Epoch:    4/10    Loss: 4.075891168117523    Least Loss: 3.966022877693176\n",
      "\n",
      "Epoch:    4/10    Loss: 4.046585707187653    Least Loss: 3.966022877693176\n",
      "\n",
      "Epoch:    4/10    Loss: 4.005922060012818    Least Loss: 3.966022877693176\n",
      "\n",
      "Epoch:    4/10    Loss: 4.05587841129303    Least Loss: 3.966022877693176\n",
      "\n",
      "Epoch:    4/10    Loss: 4.026134518146515    Least Loss: 3.966022877693176\n",
      "\n",
      "Epoch:    4/10    Loss: 4.036159752845764    Least Loss: 3.966022877693176\n",
      "\n",
      "Epoch:    4/10    Loss: 4.05386576795578    Least Loss: 3.966022877693176\n",
      "\n",
      "Epoch:    4/10    Loss: 4.0517170329093934    Least Loss: 3.966022877693176\n",
      "\n",
      "Epoch:    4/10    Loss: 4.089141280651092    Least Loss: 3.966022877693176\n",
      "\n",
      "Epoch:    4/10    Loss: 4.114402769088745    Least Loss: 3.966022877693176\n",
      "\n",
      "Epoch:    4/10    Loss: 4.082956209182739    Least Loss: 3.966022877693176\n",
      "\n",
      "Epoch:    4/10    Loss: 4.079862469673157    Least Loss: 3.966022877693176\n",
      "\n",
      "Epoch:    4/10    Loss: 4.044979389190674    Least Loss: 3.966022877693176\n",
      "\n",
      "Epoch:    4/10    Loss: 4.097348550319672    Least Loss: 3.966022877693176\n",
      "\n",
      "Epoch:    4/10    Loss: 4.098650492668152    Least Loss: 3.966022877693176\n",
      "\n",
      "Epoch:    4/10    Loss: 4.067466745376587    Least Loss: 3.966022877693176\n",
      "\n",
      "Epoch:    4/10    Loss: 4.042113983631134    Least Loss: 3.966022877693176\n",
      "\n",
      "Epoch:    4/10    Loss: 4.041406053066254    Least Loss: 3.966022877693176\n",
      "\n",
      "Epoch:    4/10    Loss: 4.081983294963837    Least Loss: 3.966022877693176\n",
      "\n",
      "Epoch:    4/10    Loss: 4.057462782382965    Least Loss: 3.966022877693176\n",
      "\n",
      "Epoch:    4/10    Loss: 4.088117270946503    Least Loss: 3.966022877693176\n",
      "\n",
      "Epoch:    4/10    Loss: 4.088010414123535    Least Loss: 3.966022877693176\n",
      "\n",
      "Epoch:    4/10    Loss: 4.123121667385101    Least Loss: 3.966022877693176\n",
      "\n",
      "Epoch:    4/10    Loss: 4.068138946533203    Least Loss: 3.966022877693176\n",
      "\n",
      "Epoch:    4/10    Loss: 4.10020354938507    Least Loss: 3.966022877693176\n",
      "\n",
      "Epoch:    4/10    Loss: 4.061064808368683    Least Loss: 3.966022877693176\n",
      "\n",
      "Epoch:    4/10    Loss: 4.091367987155914    Least Loss: 3.966022877693176\n",
      "\n",
      "Epoch:    4/10    Loss: 4.0581237177848815    Least Loss: 3.966022877693176\n",
      "\n",
      "Epoch:    4/10    Loss: 4.096876519680023    Least Loss: 3.966022877693176\n",
      "\n",
      "Epoch:    4/10    Loss: 4.099074285984039    Least Loss: 3.966022877693176\n",
      "\n",
      "Epoch:    4/10    Loss: 4.080751100063324    Least Loss: 3.966022877693176\n",
      "\n",
      "Epoch:    4/10    Loss: 4.109606506347657    Least Loss: 3.966022877693176\n",
      "\n",
      "Epoch:    4/10    Loss: 4.092426704406738    Least Loss: 3.966022877693176\n",
      "\n",
      "Epoch:    5/10    Loss: 3.997423927215785    Least Loss: 3.966022877693176\n",
      "\n",
      "Epoch:    5/10    Loss: 4.014424950122833    Least Loss: 3.966022877693176\n",
      "\n",
      "Epoch:    5/10    Loss: 3.9231679124832155    Least Loss: 3.966022877693176\n",
      "      Model Saved ..\n",
      "\n",
      "Epoch:    5/10    Loss: 3.980307671070099    Least Loss: 3.9231679124832155\n",
      "\n",
      "Epoch:    5/10    Loss: 3.9322141690254213    Least Loss: 3.9231679124832155\n",
      "\n",
      "Epoch:    5/10    Loss: 3.9302216148376465    Least Loss: 3.9231679124832155\n",
      "\n",
      "Epoch:    5/10    Loss: 3.9514741399288176    Least Loss: 3.9231679124832155\n",
      "\n",
      "Epoch:    5/10    Loss: 3.9981685738563537    Least Loss: 3.9231679124832155\n",
      "\n",
      "Epoch:    5/10    Loss: 3.8895872416496275    Least Loss: 3.9231679124832155\n",
      "      Model Saved ..\n",
      "\n",
      "Epoch:    5/10    Loss: 3.9614703879356385    Least Loss: 3.8895872416496275\n",
      "\n",
      "Epoch:    5/10    Loss: 4.013366360187531    Least Loss: 3.8895872416496275\n",
      "\n",
      "Epoch:    5/10    Loss: 3.977278269767761    Least Loss: 3.8895872416496275\n",
      "\n",
      "Epoch:    5/10    Loss: 3.9682122316360475    Least Loss: 3.8895872416496275\n",
      "\n",
      "Epoch:    5/10    Loss: 4.020273418426513    Least Loss: 3.8895872416496275\n",
      "\n",
      "Epoch:    5/10    Loss: 4.02518762922287    Least Loss: 3.8895872416496275\n",
      "\n",
      "Epoch:    5/10    Loss: 3.9952845191955566    Least Loss: 3.8895872416496275\n",
      "\n",
      "Epoch:    5/10    Loss: 3.993766601085663    Least Loss: 3.8895872416496275\n",
      "\n",
      "Epoch:    5/10    Loss: 3.9769271931648253    Least Loss: 3.8895872416496275\n",
      "\n",
      "Epoch:    5/10    Loss: 4.017716196060181    Least Loss: 3.8895872416496275\n",
      "\n",
      "Epoch:    5/10    Loss: 3.980550056934357    Least Loss: 3.8895872416496275\n",
      "\n",
      "Epoch:    5/10    Loss: 4.007096810340881    Least Loss: 3.8895872416496275\n",
      "\n",
      "Epoch:    5/10    Loss: 3.9852616696357726    Least Loss: 3.8895872416496275\n",
      "\n",
      "Epoch:    5/10    Loss: 4.027893582344055    Least Loss: 3.8895872416496275\n",
      "\n",
      "Epoch:    5/10    Loss: 4.025305036067962    Least Loss: 3.8895872416496275\n",
      "\n",
      "Epoch:    5/10    Loss: 3.9649746747016907    Least Loss: 3.8895872416496275\n",
      "\n",
      "Epoch:    5/10    Loss: 4.040707411289215    Least Loss: 3.8895872416496275\n",
      "\n",
      "Epoch:    5/10    Loss: 4.021548078536988    Least Loss: 3.8895872416496275\n",
      "\n",
      "Epoch:    5/10    Loss: 4.006194905757904    Least Loss: 3.8895872416496275\n",
      "\n",
      "Epoch:    5/10    Loss: 3.967835678577423    Least Loss: 3.8895872416496275\n",
      "\n",
      "Epoch:    5/10    Loss: 3.9968704814910887    Least Loss: 3.8895872416496275\n",
      "\n",
      "Epoch:    5/10    Loss: 4.001030725955963    Least Loss: 3.8895872416496275\n",
      "\n",
      "Epoch:    5/10    Loss: 4.019362562179565    Least Loss: 3.8895872416496275\n",
      "\n",
      "Epoch:    5/10    Loss: 3.983495219230652    Least Loss: 3.8895872416496275\n",
      "\n",
      "Epoch:    5/10    Loss: 4.047303388118744    Least Loss: 3.8895872416496275\n",
      "\n",
      "Epoch:    5/10    Loss: 3.975638593196869    Least Loss: 3.8895872416496275\n",
      "\n",
      "Epoch:    5/10    Loss: 4.021244280815124    Least Loss: 3.8895872416496275\n",
      "\n",
      "Epoch:    5/10    Loss: 4.109187917709351    Least Loss: 3.8895872416496275\n",
      "\n",
      "Epoch:    5/10    Loss: 4.031696874141693    Least Loss: 3.8895872416496275\n",
      "\n",
      "Epoch:    5/10    Loss: 4.023537306785584    Least Loss: 3.8895872416496275\n",
      "\n",
      "Epoch:    5/10    Loss: 4.027077971458435    Least Loss: 3.8895872416496275\n",
      "\n",
      "Epoch:    5/10    Loss: 4.017561191558838    Least Loss: 3.8895872416496275\n",
      "\n",
      "Epoch:    5/10    Loss: 4.000897965908051    Least Loss: 3.8895872416496275\n",
      "\n",
      "Epoch:    5/10    Loss: 4.009863237380982    Least Loss: 3.8895872416496275\n",
      "\n",
      "Epoch:    5/10    Loss: 4.004657043933868    Least Loss: 3.8895872416496275\n",
      "\n",
      "Epoch:    5/10    Loss: 4.036350708484649    Least Loss: 3.8895872416496275\n",
      "\n",
      "Epoch:    5/10    Loss: 4.098465956687927    Least Loss: 3.8895872416496275\n",
      "\n",
      "Epoch:    5/10    Loss: 3.9547885732650756    Least Loss: 3.8895872416496275\n",
      "\n",
      "Epoch:    5/10    Loss: 4.01088365316391    Least Loss: 3.8895872416496275\n",
      "\n",
      "Epoch:    5/10    Loss: 4.1202524061203    Least Loss: 3.8895872416496275\n",
      "\n",
      "Epoch:    5/10    Loss: 4.012774858951569    Least Loss: 3.8895872416496275\n",
      "\n",
      "Epoch:    5/10    Loss: 4.0897724471092225    Least Loss: 3.8895872416496275\n",
      "\n",
      "Epoch:    5/10    Loss: 4.0241376080513    Least Loss: 3.8895872416496275\n",
      "\n",
      "Epoch:    5/10    Loss: 4.048889498949051    Least Loss: 3.8895872416496275\n",
      "\n",
      "Epoch:    5/10    Loss: 3.9891884360313417    Least Loss: 3.8895872416496275\n",
      "\n",
      "Epoch:    5/10    Loss: 4.027451584815979    Least Loss: 3.8895872416496275\n",
      "\n",
      "Epoch:    6/10    Loss: 3.9804168084600224    Least Loss: 3.8895872416496275\n",
      "\n",
      "Epoch:    6/10    Loss: 3.9605145692825316    Least Loss: 3.8895872416496275\n",
      "\n",
      "Epoch:    6/10    Loss: 3.8899611144065855    Least Loss: 3.8895872416496275\n",
      "\n",
      "Epoch:    6/10    Loss: 3.8955914340019224    Least Loss: 3.8895872416496275\n",
      "\n",
      "Epoch:    6/10    Loss: 3.967266273021698    Least Loss: 3.8895872416496275\n",
      "\n",
      "Epoch:    6/10    Loss: 3.9071715760231016    Least Loss: 3.8895872416496275\n",
      "\n",
      "Epoch:    6/10    Loss: 3.9138012042045593    Least Loss: 3.8895872416496275\n",
      "\n",
      "Epoch:    6/10    Loss: 3.8855528202056884    Least Loss: 3.8895872416496275\n",
      "      Model Saved ..\n",
      "\n",
      "Epoch:    6/10    Loss: 3.919681826591492    Least Loss: 3.8855528202056884\n",
      "\n",
      "Epoch:    6/10    Loss: 3.931324646949768    Least Loss: 3.8855528202056884\n",
      "\n",
      "Epoch:    6/10    Loss: 3.943216610431671    Least Loss: 3.8855528202056884\n",
      "\n",
      "Epoch:    6/10    Loss: 3.9365800404548645    Least Loss: 3.8855528202056884\n",
      "\n",
      "Epoch:    6/10    Loss: 3.9260373191833495    Least Loss: 3.8855528202056884\n",
      "\n",
      "Epoch:    6/10    Loss: 3.959358606338501    Least Loss: 3.8855528202056884\n",
      "\n",
      "Epoch:    6/10    Loss: 3.916389796257019    Least Loss: 3.8855528202056884\n",
      "\n",
      "Epoch:    6/10    Loss: 3.945895366191864    Least Loss: 3.8855528202056884\n",
      "\n",
      "Epoch:    6/10    Loss: 3.937822392463684    Least Loss: 3.8855528202056884\n",
      "\n",
      "Epoch:    6/10    Loss: 3.9479561986923217    Least Loss: 3.8855528202056884\n",
      "\n",
      "Epoch:    6/10    Loss: 3.983854530334473    Least Loss: 3.8855528202056884\n",
      "\n",
      "Epoch:    6/10    Loss: 3.993494647979736    Least Loss: 3.8855528202056884\n",
      "\n",
      "Epoch:    6/10    Loss: 3.886100241661072    Least Loss: 3.8855528202056884\n",
      "\n",
      "Epoch:    6/10    Loss: 3.942017297744751    Least Loss: 3.8855528202056884\n",
      "\n",
      "Epoch:    6/10    Loss: 4.015831472396851    Least Loss: 3.8855528202056884\n",
      "\n",
      "Epoch:    6/10    Loss: 3.9876508927345276    Least Loss: 3.8855528202056884\n",
      "\n",
      "Epoch:    6/10    Loss: 3.947129018306732    Least Loss: 3.8855528202056884\n",
      "\n",
      "Epoch:    6/10    Loss: 4.013846612930298    Least Loss: 3.8855528202056884\n",
      "\n",
      "Epoch:    6/10    Loss: 3.9948461041450503    Least Loss: 3.8855528202056884\n",
      "\n",
      "Epoch:    6/10    Loss: 4.020465859413147    Least Loss: 3.8855528202056884\n",
      "\n",
      "Epoch:    6/10    Loss: 3.9518617367744446    Least Loss: 3.8855528202056884\n",
      "\n",
      "Epoch:    6/10    Loss: 3.9745208010673525    Least Loss: 3.8855528202056884\n",
      "\n",
      "Epoch:    6/10    Loss: 3.9363645896911623    Least Loss: 3.8855528202056884\n",
      "\n",
      "Epoch:    6/10    Loss: 3.9392232813835144    Least Loss: 3.8855528202056884\n",
      "\n",
      "Epoch:    6/10    Loss: 3.999520453453064    Least Loss: 3.8855528202056884\n",
      "\n",
      "Epoch:    6/10    Loss: 3.9683877549171447    Least Loss: 3.8855528202056884\n",
      "\n",
      "Epoch:    6/10    Loss: 4.003752038955689    Least Loss: 3.8855528202056884\n",
      "\n",
      "Epoch:    6/10    Loss: 3.9804976859092713    Least Loss: 3.8855528202056884\n",
      "\n",
      "Epoch:    6/10    Loss: 4.030875273704529    Least Loss: 3.8855528202056884\n",
      "\n",
      "Epoch:    6/10    Loss: 3.949095697879791    Least Loss: 3.8855528202056884\n",
      "\n",
      "Epoch:    6/10    Loss: 3.991561429977417    Least Loss: 3.8855528202056884\n",
      "\n",
      "Epoch:    6/10    Loss: 3.9624468870162963    Least Loss: 3.8855528202056884\n",
      "\n",
      "Epoch:    6/10    Loss: 3.9689289951324462    Least Loss: 3.8855528202056884\n",
      "\n",
      "Epoch:    6/10    Loss: 3.996298183441162    Least Loss: 3.8855528202056884\n",
      "\n",
      "Epoch:    6/10    Loss: 3.9862778730392456    Least Loss: 3.8855528202056884\n",
      "\n",
      "Epoch:    6/10    Loss: 3.954714952945709    Least Loss: 3.8855528202056884\n",
      "\n",
      "Epoch:    6/10    Loss: 4.054423537254333    Least Loss: 3.8855528202056884\n",
      "\n",
      "Epoch:    6/10    Loss: 4.057744935035705    Least Loss: 3.8855528202056884\n",
      "\n",
      "Epoch:    6/10    Loss: 3.997945333003998    Least Loss: 3.8855528202056884\n",
      "\n",
      "Epoch:    6/10    Loss: 4.0088917970657345    Least Loss: 3.8855528202056884\n",
      "\n",
      "Epoch:    6/10    Loss: 4.002777549266815    Least Loss: 3.8855528202056884\n",
      "\n",
      "Epoch:    6/10    Loss: 3.994107356786728    Least Loss: 3.8855528202056884\n",
      "\n",
      "Epoch:    6/10    Loss: 3.9788503103256225    Least Loss: 3.8855528202056884\n",
      "\n",
      "Epoch:    6/10    Loss: 4.013798499107361    Least Loss: 3.8855528202056884\n",
      "\n",
      "Epoch:    6/10    Loss: 4.012894299507141    Least Loss: 3.8855528202056884\n",
      "\n",
      "Epoch:    6/10    Loss: 4.021902193546295    Least Loss: 3.8855528202056884\n",
      "\n",
      "Epoch:    6/10    Loss: 4.033006217002868    Least Loss: 3.8855528202056884\n",
      "\n",
      "Epoch:    7/10    Loss: 3.8801833974322815    Least Loss: 3.8855528202056884\n",
      "      Model Saved ..\n",
      "\n",
      "Epoch:    7/10    Loss: 3.8733931260108947    Least Loss: 3.8801833974322815\n",
      "      Model Saved ..\n",
      "\n",
      "Epoch:    7/10    Loss: 3.8356701774597166    Least Loss: 3.8733931260108947\n",
      "      Model Saved ..\n",
      "\n",
      "Epoch:    7/10    Loss: 3.820966341972351    Least Loss: 3.8356701774597166\n",
      "      Model Saved ..\n",
      "\n",
      "Epoch:    7/10    Loss: 3.8968829770088194    Least Loss: 3.820966341972351\n",
      "\n",
      "Epoch:    7/10    Loss: 3.866294304847717    Least Loss: 3.820966341972351\n",
      "\n",
      "Epoch:    7/10    Loss: 3.888568839550018    Least Loss: 3.820966341972351\n",
      "\n",
      "Epoch:    7/10    Loss: 3.8853300232887267    Least Loss: 3.820966341972351\n",
      "\n",
      "Epoch:    7/10    Loss: 3.8668336415290834    Least Loss: 3.820966341972351\n",
      "\n",
      "Epoch:    7/10    Loss: 3.8639081621170046    Least Loss: 3.820966341972351\n",
      "\n",
      "Epoch:    7/10    Loss: 3.9000300912857058    Least Loss: 3.820966341972351\n",
      "\n",
      "Epoch:    7/10    Loss: 3.9009698672294615    Least Loss: 3.820966341972351\n",
      "\n",
      "Epoch:    7/10    Loss: 3.8851393909454344    Least Loss: 3.820966341972351\n",
      "\n",
      "Epoch:    7/10    Loss: 3.96289573431015    Least Loss: 3.820966341972351\n",
      "\n",
      "Epoch:    7/10    Loss: 3.8737092175483703    Least Loss: 3.820966341972351\n",
      "\n",
      "Epoch:    7/10    Loss: 3.939279369354248    Least Loss: 3.820966341972351\n",
      "\n",
      "Epoch:    7/10    Loss: 3.902363419532776    Least Loss: 3.820966341972351\n",
      "\n",
      "Epoch:    7/10    Loss: 3.9241520037651063    Least Loss: 3.820966341972351\n",
      "\n",
      "Epoch:    7/10    Loss: 3.8905321493148803    Least Loss: 3.820966341972351\n",
      "\n",
      "Epoch:    7/10    Loss: 3.9232140712738035    Least Loss: 3.820966341972351\n",
      "\n",
      "Epoch:    7/10    Loss: 3.9331488127708436    Least Loss: 3.820966341972351\n",
      "\n",
      "Epoch:    7/10    Loss: 3.921138569355011    Least Loss: 3.820966341972351\n",
      "\n",
      "Epoch:    7/10    Loss: 3.917284239768982    Least Loss: 3.820966341972351\n",
      "\n",
      "Epoch:    7/10    Loss: 3.9579061551094057    Least Loss: 3.820966341972351\n",
      "\n",
      "Epoch:    7/10    Loss: 3.929536470413208    Least Loss: 3.820966341972351\n",
      "\n",
      "Epoch:    7/10    Loss: 3.9443617687225343    Least Loss: 3.820966341972351\n",
      "\n",
      "Epoch:    7/10    Loss: 3.9908674988746644    Least Loss: 3.820966341972351\n",
      "\n",
      "Epoch:    7/10    Loss: 3.9541208047866823    Least Loss: 3.820966341972351\n",
      "\n",
      "Epoch:    7/10    Loss: 3.932475199699402    Least Loss: 3.820966341972351\n",
      "\n",
      "Epoch:    7/10    Loss: 3.9381779780387878    Least Loss: 3.820966341972351\n",
      "\n",
      "Epoch:    7/10    Loss: 3.9765510239601136    Least Loss: 3.820966341972351\n",
      "\n",
      "Epoch:    7/10    Loss: 3.910414473056793    Least Loss: 3.820966341972351\n",
      "\n",
      "Epoch:    7/10    Loss: 4.0206664867401125    Least Loss: 3.820966341972351\n",
      "\n",
      "Epoch:    7/10    Loss: 3.9814669539928436    Least Loss: 3.820966341972351\n",
      "\n",
      "Epoch:    7/10    Loss: 4.0230592880249025    Least Loss: 3.820966341972351\n",
      "\n",
      "Epoch:    7/10    Loss: 3.9619726362228396    Least Loss: 3.820966341972351\n",
      "\n",
      "Epoch:    7/10    Loss: 3.914865806102753    Least Loss: 3.820966341972351\n",
      "\n",
      "Epoch:    7/10    Loss: 3.9314951515197754    Least Loss: 3.820966341972351\n",
      "\n",
      "Epoch:    7/10    Loss: 3.972735858917236    Least Loss: 3.820966341972351\n",
      "\n",
      "Epoch:    7/10    Loss: 3.980769995212555    Least Loss: 3.820966341972351\n",
      "\n",
      "Epoch:    7/10    Loss: 3.9665994234085082    Least Loss: 3.820966341972351\n",
      "\n",
      "Epoch:    7/10    Loss: 3.9631998448371886    Least Loss: 3.820966341972351\n",
      "\n",
      "Epoch:    7/10    Loss: 3.9147411630153655    Least Loss: 3.820966341972351\n",
      "\n",
      "Epoch:    7/10    Loss: 3.9460664601325988    Least Loss: 3.820966341972351\n",
      "\n",
      "Epoch:    7/10    Loss: 3.9160752902030946    Least Loss: 3.820966341972351\n",
      "\n",
      "Epoch:    7/10    Loss: 3.972402680158615    Least Loss: 3.820966341972351\n",
      "\n",
      "Epoch:    7/10    Loss: 3.9881670444011688    Least Loss: 3.820966341972351\n",
      "\n",
      "Epoch:    7/10    Loss: 3.999965240955353    Least Loss: 3.820966341972351\n",
      "\n",
      "Epoch:    7/10    Loss: 3.9581039180755617    Least Loss: 3.820966341972351\n",
      "\n",
      "Epoch:    7/10    Loss: 3.9077355031967165    Least Loss: 3.820966341972351\n",
      "\n",
      "Epoch:    7/10    Loss: 3.93227370929718    Least Loss: 3.820966341972351\n",
      "\n",
      "Epoch:    7/10    Loss: 3.9432925691604614    Least Loss: 3.820966341972351\n",
      "\n",
      "Epoch:    7/10    Loss: 3.9776612706184387    Least Loss: 3.820966341972351\n",
      "\n",
      "Epoch:    7/10    Loss: 3.9814537224769593    Least Loss: 3.820966341972351\n",
      "\n",
      "Epoch:    7/10    Loss: 3.9607014017105104    Least Loss: 3.820966341972351\n",
      "\n",
      "Epoch:    8/10    Loss: 3.897889659777199    Least Loss: 3.820966341972351\n",
      "\n",
      "Epoch:    8/10    Loss: 3.87924050283432    Least Loss: 3.820966341972351\n",
      "\n",
      "Epoch:    8/10    Loss: 3.7876966290473937    Least Loss: 3.820966341972351\n",
      "      Model Saved ..\n",
      "\n",
      "Epoch:    8/10    Loss: 3.8185816049575805    Least Loss: 3.7876966290473937\n",
      "\n",
      "Epoch:    8/10    Loss: 3.8211754665374755    Least Loss: 3.7876966290473937\n",
      "\n",
      "Epoch:    8/10    Loss: 3.851109091281891    Least Loss: 3.7876966290473937\n",
      "\n",
      "Epoch:    8/10    Loss: 3.7812280447483064    Least Loss: 3.7876966290473937\n",
      "      Model Saved ..\n",
      "\n",
      "Epoch:    8/10    Loss: 3.8090142369270326    Least Loss: 3.7812280447483064\n",
      "\n",
      "Epoch:    8/10    Loss: 3.8235300741195677    Least Loss: 3.7812280447483064\n",
      "\n",
      "Epoch:    8/10    Loss: 3.80691708946228    Least Loss: 3.7812280447483064\n",
      "\n",
      "Epoch:    8/10    Loss: 3.876577802181244    Least Loss: 3.7812280447483064\n",
      "\n",
      "Epoch:    8/10    Loss: 3.9270211567878723    Least Loss: 3.7812280447483064\n",
      "\n",
      "Epoch:    8/10    Loss: 3.835180875778198    Least Loss: 3.7812280447483064\n",
      "\n",
      "Epoch:    8/10    Loss: 3.862571885108948    Least Loss: 3.7812280447483064\n",
      "\n",
      "Epoch:    8/10    Loss: 3.9131794724464415    Least Loss: 3.7812280447483064\n",
      "\n",
      "Epoch:    8/10    Loss: 3.818240637779236    Least Loss: 3.7812280447483064\n",
      "\n",
      "Epoch:    8/10    Loss: 3.837795141220093    Least Loss: 3.7812280447483064\n",
      "\n",
      "Epoch:    8/10    Loss: 3.8910594959259033    Least Loss: 3.7812280447483064\n",
      "\n",
      "Epoch:    8/10    Loss: 3.883006413936615    Least Loss: 3.7812280447483064\n",
      "\n",
      "Epoch:    8/10    Loss: 3.871417989730835    Least Loss: 3.7812280447483064\n",
      "\n",
      "Epoch:    8/10    Loss: 3.878382747173309    Least Loss: 3.7812280447483064\n",
      "\n",
      "Epoch:    8/10    Loss: 3.8486254246234894    Least Loss: 3.7812280447483064\n",
      "\n",
      "Epoch:    8/10    Loss: 3.953273337841034    Least Loss: 3.7812280447483064\n",
      "\n",
      "Epoch:    8/10    Loss: 3.8824935631752013    Least Loss: 3.7812280447483064\n",
      "\n",
      "Epoch:    8/10    Loss: 3.873623846292496    Least Loss: 3.7812280447483064\n",
      "\n",
      "Epoch:    8/10    Loss: 3.8841400175094605    Least Loss: 3.7812280447483064\n",
      "\n",
      "Epoch:    8/10    Loss: 3.8936379442214966    Least Loss: 3.7812280447483064\n",
      "\n",
      "Epoch:    8/10    Loss: 3.9480039982795714    Least Loss: 3.7812280447483064\n",
      "\n",
      "Epoch:    8/10    Loss: 3.9251313281059264    Least Loss: 3.7812280447483064\n",
      "\n",
      "Epoch:    8/10    Loss: 3.9536228585243225    Least Loss: 3.7812280447483064\n",
      "\n",
      "Epoch:    8/10    Loss: 3.9164386086463927    Least Loss: 3.7812280447483064\n",
      "\n",
      "Epoch:    8/10    Loss: 3.854827615737915    Least Loss: 3.7812280447483064\n",
      "\n",
      "Epoch:    8/10    Loss: 3.918812783241272    Least Loss: 3.7812280447483064\n",
      "\n",
      "Epoch:    8/10    Loss: 4.028765200614929    Least Loss: 3.7812280447483064\n",
      "\n",
      "Epoch:    8/10    Loss: 3.914025312423706    Least Loss: 3.7812280447483064\n",
      "\n",
      "Epoch:    8/10    Loss: 3.9270002908706667    Least Loss: 3.7812280447483064\n",
      "\n",
      "Epoch:    8/10    Loss: 3.901201078891754    Least Loss: 3.7812280447483064\n",
      "\n",
      "Epoch:    8/10    Loss: 3.9920230531692504    Least Loss: 3.7812280447483064\n",
      "\n",
      "Epoch:    8/10    Loss: 3.9420258169174196    Least Loss: 3.7812280447483064\n",
      "\n",
      "Epoch:    8/10    Loss: 3.89555073595047    Least Loss: 3.7812280447483064\n",
      "\n",
      "Epoch:    8/10    Loss: 3.8752022924423217    Least Loss: 3.7812280447483064\n",
      "\n",
      "Epoch:    8/10    Loss: 3.9260875391960144    Least Loss: 3.7812280447483064\n",
      "\n",
      "Epoch:    8/10    Loss: 3.922215000629425    Least Loss: 3.7812280447483064\n",
      "\n",
      "Epoch:    8/10    Loss: 3.890087655544281    Least Loss: 3.7812280447483064\n",
      "\n",
      "Epoch:    8/10    Loss: 3.94179100894928    Least Loss: 3.7812280447483064\n",
      "\n",
      "Epoch:    8/10    Loss: 3.974724362850189    Least Loss: 3.7812280447483064\n",
      "\n",
      "Epoch:    8/10    Loss: 3.9480588092803957    Least Loss: 3.7812280447483064\n",
      "\n",
      "Epoch:    8/10    Loss: 3.973085837364197    Least Loss: 3.7812280447483064\n",
      "\n",
      "Epoch:    8/10    Loss: 3.9272587413787843    Least Loss: 3.7812280447483064\n",
      "\n",
      "Epoch:    8/10    Loss: 3.925816891670227    Least Loss: 3.7812280447483064\n",
      "\n",
      "Epoch:    8/10    Loss: 4.008153159618377    Least Loss: 3.7812280447483064\n",
      "\n",
      "Epoch:    8/10    Loss: 3.937843448638916    Least Loss: 3.7812280447483064\n",
      "\n",
      "Epoch:    8/10    Loss: 3.919554475784302    Least Loss: 3.7812280447483064\n",
      "\n",
      "Epoch:    8/10    Loss: 3.9762003870010374    Least Loss: 3.7812280447483064\n",
      "\n",
      "Epoch:    8/10    Loss: 3.9568697688579557    Least Loss: 3.7812280447483064\n",
      "\n",
      "Epoch:    9/10    Loss: 3.822949517683945    Least Loss: 3.7812280447483064\n",
      "\n",
      "Epoch:    9/10    Loss: 3.8089310455322267    Least Loss: 3.7812280447483064\n",
      "\n",
      "Epoch:    9/10    Loss: 3.8179981100559233    Least Loss: 3.7812280447483064\n",
      "\n",
      "Epoch:    9/10    Loss: 3.7674815607070924    Least Loss: 3.7812280447483064\n",
      "      Model Saved ..\n",
      "\n",
      "Epoch:    9/10    Loss: 3.845709008216858    Least Loss: 3.7674815607070924\n",
      "\n",
      "Epoch:    9/10    Loss: 3.7712493677139283    Least Loss: 3.7674815607070924\n",
      "\n",
      "Epoch:    9/10    Loss: 3.7453524622917174    Least Loss: 3.7674815607070924\n",
      "      Model Saved ..\n",
      "\n",
      "Epoch:    9/10    Loss: 3.730080557346344    Least Loss: 3.7453524622917174\n",
      "      Model Saved ..\n",
      "\n",
      "Epoch:    9/10    Loss: 3.81005694770813    Least Loss: 3.730080557346344\n",
      "\n",
      "Epoch:    9/10    Loss: 3.8398083913326264    Least Loss: 3.730080557346344\n",
      "\n",
      "Epoch:    9/10    Loss: 3.847629477977753    Least Loss: 3.730080557346344\n",
      "\n",
      "Epoch:    9/10    Loss: 3.7999247126579285    Least Loss: 3.730080557346344\n",
      "\n",
      "Epoch:    9/10    Loss: 3.8450934970378876    Least Loss: 3.730080557346344\n",
      "\n",
      "Epoch:    9/10    Loss: 3.788805614233017    Least Loss: 3.730080557346344\n",
      "\n",
      "Epoch:    9/10    Loss: 3.8584925150871277    Least Loss: 3.730080557346344\n",
      "\n",
      "Epoch:    9/10    Loss: 3.830355350017548    Least Loss: 3.730080557346344\n",
      "\n",
      "Epoch:    9/10    Loss: 3.8134832334518434    Least Loss: 3.730080557346344\n",
      "\n",
      "Epoch:    9/10    Loss: 3.8129615993499755    Least Loss: 3.730080557346344\n",
      "\n",
      "Epoch:    9/10    Loss: 3.8792907662391665    Least Loss: 3.730080557346344\n",
      "\n",
      "Epoch:    9/10    Loss: 3.8363651645183565    Least Loss: 3.730080557346344\n",
      "\n",
      "Epoch:    9/10    Loss: 3.8113980422019957    Least Loss: 3.730080557346344\n",
      "\n",
      "Epoch:    9/10    Loss: 3.862543410778046    Least Loss: 3.730080557346344\n",
      "\n",
      "Epoch:    9/10    Loss: 3.8589233832359313    Least Loss: 3.730080557346344\n",
      "\n",
      "Epoch:    9/10    Loss: 3.855792368888855    Least Loss: 3.730080557346344\n",
      "\n",
      "Epoch:    9/10    Loss: 3.878506375312805    Least Loss: 3.730080557346344\n",
      "\n",
      "Epoch:    9/10    Loss: 3.9033086524009706    Least Loss: 3.730080557346344\n",
      "\n",
      "Epoch:    9/10    Loss: 3.8561662187576293    Least Loss: 3.730080557346344\n",
      "\n",
      "Epoch:    9/10    Loss: 3.875200770139694    Least Loss: 3.730080557346344\n",
      "\n",
      "Epoch:    9/10    Loss: 3.9017847065925597    Least Loss: 3.730080557346344\n",
      "\n",
      "Epoch:    9/10    Loss: 3.858686830997467    Least Loss: 3.730080557346344\n",
      "\n",
      "Epoch:    9/10    Loss: 3.899761663913727    Least Loss: 3.730080557346344\n",
      "\n",
      "Epoch:    9/10    Loss: 3.8538257007598875    Least Loss: 3.730080557346344\n",
      "\n",
      "Epoch:    9/10    Loss: 3.912375774383545    Least Loss: 3.730080557346344\n",
      "\n",
      "Epoch:    9/10    Loss: 3.8839393532276154    Least Loss: 3.730080557346344\n",
      "\n",
      "Epoch:    9/10    Loss: 3.9088642568588257    Least Loss: 3.730080557346344\n",
      "\n",
      "Epoch:    9/10    Loss: 3.875565032482147    Least Loss: 3.730080557346344\n",
      "\n",
      "Epoch:    9/10    Loss: 3.8995430135726927    Least Loss: 3.730080557346344\n",
      "\n",
      "Epoch:    9/10    Loss: 3.8587309131622316    Least Loss: 3.730080557346344\n",
      "\n",
      "Epoch:    9/10    Loss: 3.89373410987854    Least Loss: 3.730080557346344\n",
      "\n",
      "Epoch:    9/10    Loss: 3.8655724358558654    Least Loss: 3.730080557346344\n",
      "\n",
      "Epoch:    9/10    Loss: 3.9290848128795623    Least Loss: 3.730080557346344\n",
      "\n",
      "Epoch:    9/10    Loss: 3.942491623878479    Least Loss: 3.730080557346344\n",
      "\n",
      "Epoch:    9/10    Loss: 3.8947422914505005    Least Loss: 3.730080557346344\n",
      "\n",
      "Epoch:    9/10    Loss: 3.937752583026886    Least Loss: 3.730080557346344\n",
      "\n",
      "Epoch:    9/10    Loss: 3.9465961194038393    Least Loss: 3.730080557346344\n",
      "\n",
      "Epoch:    9/10    Loss: 3.863673509597778    Least Loss: 3.730080557346344\n",
      "\n",
      "Epoch:    9/10    Loss: 3.950191162109375    Least Loss: 3.730080557346344\n",
      "\n",
      "Epoch:    9/10    Loss: 3.920171633720398    Least Loss: 3.730080557346344\n",
      "\n",
      "Epoch:    9/10    Loss: 3.910711360454559    Least Loss: 3.730080557346344\n",
      "\n",
      "Epoch:    9/10    Loss: 3.9271156063079835    Least Loss: 3.730080557346344\n",
      "\n",
      "Epoch:    9/10    Loss: 3.8828063497543335    Least Loss: 3.730080557346344\n",
      "\n",
      "Epoch:    9/10    Loss: 3.905852031707764    Least Loss: 3.730080557346344\n",
      "\n",
      "Epoch:    9/10    Loss: 3.874351828575134    Least Loss: 3.730080557346344\n",
      "\n",
      "Epoch:    9/10    Loss: 3.8892653355598448    Least Loss: 3.730080557346344\n",
      "\n",
      "Epoch:    9/10    Loss: 3.92468554353714    Least Loss: 3.730080557346344\n",
      "\n",
      "Epoch:   10/10    Loss: 3.8295764629490154    Least Loss: 3.730080557346344\n",
      "\n",
      "Epoch:   10/10    Loss: 3.7453514499664307    Least Loss: 3.730080557346344\n",
      "\n",
      "Epoch:   10/10    Loss: 3.7907760455608366    Least Loss: 3.730080557346344\n",
      "\n",
      "Epoch:   10/10    Loss: 3.755385057926178    Least Loss: 3.730080557346344\n",
      "\n",
      "Epoch:   10/10    Loss: 3.762511184215546    Least Loss: 3.730080557346344\n",
      "\n",
      "Epoch:   10/10    Loss: 3.752937936782837    Least Loss: 3.730080557346344\n",
      "\n",
      "Epoch:   10/10    Loss: 3.8012703051567076    Least Loss: 3.730080557346344\n",
      "\n",
      "Epoch:   10/10    Loss: 3.765045777797699    Least Loss: 3.730080557346344\n",
      "\n",
      "Epoch:   10/10    Loss: 3.7763259110450744    Least Loss: 3.730080557346344\n",
      "\n",
      "Epoch:   10/10    Loss: 3.7751003520488737    Least Loss: 3.730080557346344\n",
      "\n",
      "Epoch:   10/10    Loss: 3.783476784706116    Least Loss: 3.730080557346344\n",
      "\n",
      "Epoch:   10/10    Loss: 3.8151374092102053    Least Loss: 3.730080557346344\n",
      "\n",
      "Epoch:   10/10    Loss: 3.801839856863022    Least Loss: 3.730080557346344\n",
      "\n",
      "Epoch:   10/10    Loss: 3.828154402256012    Least Loss: 3.730080557346344\n",
      "\n",
      "Epoch:   10/10    Loss: 3.809182728290558    Least Loss: 3.730080557346344\n",
      "\n",
      "Epoch:   10/10    Loss: 3.768464708328247    Least Loss: 3.730080557346344\n",
      "\n",
      "Epoch:   10/10    Loss: 3.871818805217743    Least Loss: 3.730080557346344\n",
      "\n",
      "Epoch:   10/10    Loss: 3.8678791480064394    Least Loss: 3.730080557346344\n",
      "\n",
      "Epoch:   10/10    Loss: 3.7951995415687563    Least Loss: 3.730080557346344\n",
      "\n",
      "Epoch:   10/10    Loss: 3.797381501674652    Least Loss: 3.730080557346344\n",
      "\n",
      "Epoch:   10/10    Loss: 3.8241626138687135    Least Loss: 3.730080557346344\n",
      "\n",
      "Epoch:   10/10    Loss: 3.8136238043308257    Least Loss: 3.730080557346344\n",
      "\n",
      "Epoch:   10/10    Loss: 3.8324644503593444    Least Loss: 3.730080557346344\n",
      "\n",
      "Epoch:   10/10    Loss: 3.8576604273319246    Least Loss: 3.730080557346344\n",
      "\n",
      "Epoch:   10/10    Loss: 3.832401505470276    Least Loss: 3.730080557346344\n",
      "\n",
      "Epoch:   10/10    Loss: 3.834714903354645    Least Loss: 3.730080557346344\n",
      "\n",
      "Epoch:   10/10    Loss: 3.880775209188461    Least Loss: 3.730080557346344\n",
      "\n",
      "Epoch:   10/10    Loss: 3.869058511734009    Least Loss: 3.730080557346344\n",
      "\n",
      "Epoch:   10/10    Loss: 3.8488967123031617    Least Loss: 3.730080557346344\n",
      "\n",
      "Epoch:   10/10    Loss: 3.7935629954338075    Least Loss: 3.730080557346344\n",
      "\n",
      "Epoch:   10/10    Loss: 3.841919490814209    Least Loss: 3.730080557346344\n",
      "\n",
      "Epoch:   10/10    Loss: 3.8718642392158507    Least Loss: 3.730080557346344\n",
      "\n",
      "Epoch:   10/10    Loss: 3.82097318482399    Least Loss: 3.730080557346344\n",
      "\n",
      "Epoch:   10/10    Loss: 3.827859221935272    Least Loss: 3.730080557346344\n",
      "\n",
      "Epoch:   10/10    Loss: 3.9088727207183838    Least Loss: 3.730080557346344\n",
      "\n",
      "Epoch:   10/10    Loss: 3.8493556265830993    Least Loss: 3.730080557346344\n",
      "\n",
      "Epoch:   10/10    Loss: 3.844885865211487    Least Loss: 3.730080557346344\n",
      "\n",
      "Epoch:   10/10    Loss: 3.885240469455719    Least Loss: 3.730080557346344\n",
      "\n",
      "Epoch:   10/10    Loss: 3.877440239906311    Least Loss: 3.730080557346344\n",
      "\n",
      "Epoch:   10/10    Loss: 3.8715590472221373    Least Loss: 3.730080557346344\n",
      "\n",
      "Epoch:   10/10    Loss: 3.8887700152397158    Least Loss: 3.730080557346344\n",
      "\n",
      "Epoch:   10/10    Loss: 3.839251197099686    Least Loss: 3.730080557346344\n",
      "\n",
      "Epoch:   10/10    Loss: 3.810926395654678    Least Loss: 3.730080557346344\n",
      "\n",
      "Epoch:   10/10    Loss: 3.8089480447769164    Least Loss: 3.730080557346344\n",
      "\n",
      "Epoch:   10/10    Loss: 3.8472338309288023    Least Loss: 3.730080557346344\n",
      "\n",
      "Epoch:   10/10    Loss: 3.8830824117660523    Least Loss: 3.730080557346344\n",
      "\n",
      "Epoch:   10/10    Loss: 3.9261920137405397    Least Loss: 3.730080557346344\n",
      "\n",
      "Epoch:   10/10    Loss: 3.9409483880996703    Least Loss: 3.730080557346344\n",
      "\n",
      "Epoch:   10/10    Loss: 3.8955517439842224    Least Loss: 3.730080557346344\n",
      "\n",
      "Epoch:   10/10    Loss: 3.845933324813843    Least Loss: 3.730080557346344\n",
      "\n",
      "Epoch:   10/10    Loss: 3.9120203442573547    Least Loss: 3.730080557346344\n",
      "\n",
      "Epoch:   10/10    Loss: 3.919731044769287    Least Loss: 3.730080557346344\n",
      "\n",
      "Epoch:   10/10    Loss: 3.8979018683433533    Least Loss: 3.730080557346344\n",
      "\n",
      "Epoch:   10/10    Loss: 3.8435057969093323    Least Loss: 3.730080557346344\n",
      "\n",
      "Epoch:   10/10    Loss: 3.931324294567108    Least Loss: 3.730080557346344\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-c261a1a50802>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mactive_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtrained_rnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_rnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_every_n_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# saving the trained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-ea36442db79f>\u001b[0m in \u001b[0;36mtrain_rnn\u001b[0;34m(rnn, batch_size, optimizer, criterion, n_epochs, show_every_n_batches)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;31m# forward, back prop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_back_prop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0;31m# record loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mbatch_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-401fa8180f13>\u001b[0m in \u001b[0;36mforward_back_prop\u001b[0;34m(rnn, optimizer, criterion, inp, target, hidden)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mparam_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0mtotal_norm\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mparam_norm\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_norm\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rnn.cuda()\n",
    "with active_session():\n",
    "    trained_rnn = train_rnn(rnn, batch_size, optimizer, criterion, num_epochs, show_every_n_batches)\n",
    "\n",
    "print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 10 epoch(s)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:43: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:    1/10    Loss: 3.575888607263565    Least Loss: inf\n",
      "      Model Saved ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/torch/serialization.py:193: UserWarning: Couldn't retrieve source code for container of type RNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:    1/10    Loss: 3.5422069754600525    Least Loss: 3.575888607263565\n",
      "      Model Saved ..\n",
      "\n",
      "Epoch:    1/10    Loss: 3.5958608446121216    Least Loss: 3.5422069754600525\n",
      "\n",
      "Epoch:    1/10    Loss: 3.5870462975502013    Least Loss: 3.5422069754600525\n",
      "\n",
      "Epoch:    1/10    Loss: 3.559429311275482    Least Loss: 3.5422069754600525\n",
      "\n",
      "Epoch:    1/10    Loss: 3.6060088856220247    Least Loss: 3.5422069754600525\n",
      "\n",
      "Epoch:    1/10    Loss: 3.624845173358917    Least Loss: 3.5422069754600525\n",
      "\n",
      "Epoch:    1/10    Loss: 3.609226765155792    Least Loss: 3.5422069754600525\n",
      "\n",
      "Epoch:    1/10    Loss: 3.614244008779526    Least Loss: 3.5422069754600525\n",
      "\n",
      "Epoch:    1/10    Loss: 3.5939118304252626    Least Loss: 3.5422069754600525\n",
      "\n",
      "Epoch:    1/10    Loss: 3.574021764278412    Least Loss: 3.5422069754600525\n",
      "\n",
      "Epoch:    1/10    Loss: 3.5779249234199524    Least Loss: 3.5422069754600525\n",
      "\n",
      "Epoch:    1/10    Loss: 3.5675533690452577    Least Loss: 3.5422069754600525\n",
      "\n",
      "Epoch:    1/10    Loss: 3.5917072634696963    Least Loss: 3.5422069754600525\n",
      "\n",
      "Epoch:    1/10    Loss: 3.5553855910301206    Least Loss: 3.5422069754600525\n",
      "\n",
      "Epoch:    1/10    Loss: 3.5959105756282805    Least Loss: 3.5422069754600525\n",
      "\n",
      "Epoch:    1/10    Loss: 3.6272880573272706    Least Loss: 3.5422069754600525\n",
      "\n",
      "Epoch:    1/10    Loss: 3.5801536309719086    Least Loss: 3.5422069754600525\n",
      "\n",
      "Epoch:    1/10    Loss: 3.587519460201263    Least Loss: 3.5422069754600525\n",
      "\n",
      "Epoch:    1/10    Loss: 3.622373107433319    Least Loss: 3.5422069754600525\n",
      "\n",
      "Epoch:    1/10    Loss: 3.6156109354496    Least Loss: 3.5422069754600525\n",
      "\n",
      "Epoch:    1/10    Loss: 3.623012999296188    Least Loss: 3.5422069754600525\n",
      "\n",
      "Epoch:    1/10    Loss: 3.649636669397354    Least Loss: 3.5422069754600525\n",
      "\n",
      "Epoch:    1/10    Loss: 3.6303048100471496    Least Loss: 3.5422069754600525\n",
      "\n",
      "Epoch:    1/10    Loss: 3.5167738637924195    Least Loss: 3.5422069754600525\n",
      "      Model Saved ..\n",
      "\n",
      "Epoch:    1/10    Loss: 3.6090826296806338    Least Loss: 3.5167738637924195\n",
      "\n",
      "Epoch:    1/10    Loss: 3.6332702448368073    Least Loss: 3.5167738637924195\n",
      "\n",
      "Epoch:    1/10    Loss: 3.6099317111968996    Least Loss: 3.5167738637924195\n",
      "\n",
      "Epoch:    1/10    Loss: 3.57226681804657    Least Loss: 3.5167738637924195\n",
      "\n",
      "Epoch:    1/10    Loss: 3.614601728439331    Least Loss: 3.5167738637924195\n",
      "\n",
      "Epoch:    1/10    Loss: 3.569645368099213    Least Loss: 3.5167738637924195\n",
      "\n",
      "Epoch:    1/10    Loss: 3.5915147981643676    Least Loss: 3.5167738637924195\n",
      "\n",
      "Epoch:    1/10    Loss: 3.56712926030159    Least Loss: 3.5167738637924195\n",
      "\n",
      "Epoch:    1/10    Loss: 3.622524211883545    Least Loss: 3.5167738637924195\n",
      "\n",
      "Epoch:    1/10    Loss: 3.584454638004303    Least Loss: 3.5167738637924195\n",
      "\n",
      "Epoch:    1/10    Loss: 3.6425243673324585    Least Loss: 3.5167738637924195\n",
      "\n",
      "Epoch:    1/10    Loss: 3.632473088979721    Least Loss: 3.5167738637924195\n",
      "\n",
      "Epoch:    1/10    Loss: 3.609988839149475    Least Loss: 3.5167738637924195\n",
      "\n",
      "Epoch:    1/10    Loss: 3.6259627492427824    Least Loss: 3.5167738637924195\n",
      "\n",
      "Epoch:    1/10    Loss: 3.600735251903534    Least Loss: 3.5167738637924195\n",
      "\n",
      "Epoch:    1/10    Loss: 3.6070600788593294    Least Loss: 3.5167738637924195\n",
      "\n",
      "Epoch:    1/10    Loss: 3.6283585777282714    Least Loss: 3.5167738637924195\n",
      "\n",
      "Epoch:    1/10    Loss: 3.637792843103409    Least Loss: 3.5167738637924195\n",
      "\n",
      "Epoch:    1/10    Loss: 3.602596277475357    Least Loss: 3.5167738637924195\n",
      "\n",
      "Epoch:    1/10    Loss: 3.627558435201645    Least Loss: 3.5167738637924195\n",
      "\n",
      "Epoch:    1/10    Loss: 3.610706467628479    Least Loss: 3.5167738637924195\n",
      "\n",
      "Epoch:    1/10    Loss: 3.6419280247688293    Least Loss: 3.5167738637924195\n",
      "\n",
      "Epoch:    1/10    Loss: 3.6156850361824038    Least Loss: 3.5167738637924195\n",
      "\n",
      "Epoch:    1/10    Loss: 3.6621657981872557    Least Loss: 3.5167738637924195\n",
      "\n",
      "Epoch:    1/10    Loss: 3.6493886601924896    Least Loss: 3.5167738637924195\n",
      "\n",
      "Epoch:    1/10    Loss: 3.6024257650375366    Least Loss: 3.5167738637924195\n",
      "\n",
      "Epoch:    1/10    Loss: 3.63808842086792    Least Loss: 3.5167738637924195\n",
      "\n",
      "Epoch:    1/10    Loss: 3.61002637052536    Least Loss: 3.5167738637924195\n",
      "\n",
      "Epoch:    1/10    Loss: 3.652451672554016    Least Loss: 3.5167738637924195\n",
      "\n",
      "Epoch:    1/10    Loss: 3.6417156705856324    Least Loss: 3.5167738637924195\n",
      "\n",
      "Epoch:    2/10    Loss: 3.5885331864503796    Least Loss: 3.5167738637924195\n",
      "\n",
      "Epoch:    2/10    Loss: 3.559450471162796    Least Loss: 3.5167738637924195\n",
      "\n",
      "Epoch:    2/10    Loss: 3.5914521136283875    Least Loss: 3.5167738637924195\n",
      "\n",
      "Epoch:    2/10    Loss: 3.598743638038635    Least Loss: 3.5167738637924195\n",
      "\n",
      "Epoch:    2/10    Loss: 3.5685914294719696    Least Loss: 3.5167738637924195\n",
      "\n",
      "Epoch:    2/10    Loss: 3.569168221473694    Least Loss: 3.5167738637924195\n",
      "\n",
      "Epoch:    2/10    Loss: 3.5927415022850036    Least Loss: 3.5167738637924195\n",
      "\n",
      "Epoch:    2/10    Loss: 3.5209818091392515    Least Loss: 3.5167738637924195\n",
      "\n",
      "Epoch:    2/10    Loss: 3.563513258457184    Least Loss: 3.5167738637924195\n",
      "\n",
      "Epoch:    2/10    Loss: 3.5444378933906555    Least Loss: 3.5167738637924195\n",
      "\n",
      "Epoch:    2/10    Loss: 3.5602658557891846    Least Loss: 3.5167738637924195\n",
      "\n",
      "Epoch:    2/10    Loss: 3.567415828227997    Least Loss: 3.5167738637924195\n",
      "\n",
      "Epoch:    2/10    Loss: 3.5778618273735048    Least Loss: 3.5167738637924195\n",
      "\n",
      "Epoch:    2/10    Loss: 3.5547688553333283    Least Loss: 3.5167738637924195\n",
      "\n",
      "Epoch:    2/10    Loss: 3.568027178287506    Least Loss: 3.5167738637924195\n",
      "\n",
      "Epoch:    2/10    Loss: 3.6008975400924683    Least Loss: 3.5167738637924195\n",
      "\n",
      "Epoch:    2/10    Loss: 3.5948246722221375    Least Loss: 3.5167738637924195\n",
      "\n",
      "Epoch:    2/10    Loss: 3.563891046047211    Least Loss: 3.5167738637924195\n",
      "\n",
      "Epoch:    2/10    Loss: 3.574545661449432    Least Loss: 3.5167738637924195\n",
      "\n",
      "Epoch:    2/10    Loss: 3.5119603033065796    Least Loss: 3.5167738637924195\n",
      "      Model Saved ..\n",
      "\n",
      "Epoch:    2/10    Loss: 3.5666539907455443    Least Loss: 3.5119603033065796\n",
      "\n",
      "Epoch:    2/10    Loss: 3.5719243550300597    Least Loss: 3.5119603033065796\n",
      "\n",
      "Epoch:    2/10    Loss: 3.5547174069881438    Least Loss: 3.5119603033065796\n",
      "\n",
      "Epoch:    2/10    Loss: 3.5646814901828767    Least Loss: 3.5119603033065796\n",
      "\n",
      "Epoch:    2/10    Loss: 3.593931491613388    Least Loss: 3.5119603033065796\n",
      "\n",
      "Epoch:    2/10    Loss: 3.5907237462997434    Least Loss: 3.5119603033065796\n",
      "\n",
      "Epoch:    2/10    Loss: 3.544381854772568    Least Loss: 3.5119603033065796\n",
      "\n",
      "Epoch:    2/10    Loss: 3.5865722360610963    Least Loss: 3.5119603033065796\n",
      "\n",
      "Epoch:    2/10    Loss: 3.6422450468540193    Least Loss: 3.5119603033065796\n",
      "\n",
      "Epoch:    2/10    Loss: 3.5770055105686187    Least Loss: 3.5119603033065796\n",
      "\n",
      "Epoch:    2/10    Loss: 3.5238220038414    Least Loss: 3.5119603033065796\n",
      "\n",
      "Epoch:    2/10    Loss: 3.6026024641990664    Least Loss: 3.5119603033065796\n",
      "\n",
      "Epoch:    2/10    Loss: 3.6111552591323854    Least Loss: 3.5119603033065796\n",
      "\n",
      "Epoch:    2/10    Loss: 3.596144418478012    Least Loss: 3.5119603033065796\n",
      "\n",
      "Epoch:    2/10    Loss: 3.625540162086487    Least Loss: 3.5119603033065796\n",
      "\n",
      "Epoch:    2/10    Loss: 3.5561840052604676    Least Loss: 3.5119603033065796\n",
      "\n",
      "Epoch:    2/10    Loss: 3.618690732717514    Least Loss: 3.5119603033065796\n",
      "\n",
      "Epoch:    2/10    Loss: 3.5327848510742186    Least Loss: 3.5119603033065796\n",
      "\n",
      "Epoch:    2/10    Loss: 3.576128352165222    Least Loss: 3.5119603033065796\n",
      "\n",
      "Epoch:    2/10    Loss: 3.572891324043274    Least Loss: 3.5119603033065796\n",
      "\n",
      "Epoch:    2/10    Loss: 3.593700697660446    Least Loss: 3.5119603033065796\n",
      "\n",
      "Epoch:    2/10    Loss: 3.610816022634506    Least Loss: 3.5119603033065796\n",
      "\n",
      "Epoch:    2/10    Loss: 3.5919820971488954    Least Loss: 3.5119603033065796\n",
      "\n",
      "Epoch:    2/10    Loss: 3.5897704367637635    Least Loss: 3.5119603033065796\n",
      "\n",
      "Epoch:    2/10    Loss: 3.6020364174842836    Least Loss: 3.5119603033065796\n",
      "\n",
      "Epoch:    2/10    Loss: 3.589322687625885    Least Loss: 3.5119603033065796\n",
      "\n",
      "Epoch:    2/10    Loss: 3.626491864681244    Least Loss: 3.5119603033065796\n",
      "\n",
      "Epoch:    2/10    Loss: 3.623199128627777    Least Loss: 3.5119603033065796\n",
      "\n",
      "Epoch:    2/10    Loss: 3.6326473047733305    Least Loss: 3.5119603033065796\n",
      "\n",
      "Epoch:    2/10    Loss: 3.531258392333984    Least Loss: 3.5119603033065796\n",
      "\n",
      "Epoch:    2/10    Loss: 3.5836000576019287    Least Loss: 3.5119603033065796\n",
      "\n",
      "Epoch:    2/10    Loss: 3.641250579357147    Least Loss: 3.5119603033065796\n",
      "\n",
      "Epoch:    2/10    Loss: 3.60227023267746    Least Loss: 3.5119603033065796\n",
      "\n",
      "Epoch:    2/10    Loss: 3.6720937061309815    Least Loss: 3.5119603033065796\n",
      "\n",
      "Epoch:    2/10    Loss: 3.618374342441559    Least Loss: 3.5119603033065796\n",
      "\n",
      "Epoch:    3/10    Loss: 3.5444569275071984    Least Loss: 3.5119603033065796\n",
      "\n",
      "Epoch:    3/10    Loss: 3.5254934515953065    Least Loss: 3.5119603033065796\n",
      "\n",
      "Epoch:    3/10    Loss: 3.527672924041748    Least Loss: 3.5119603033065796\n",
      "\n",
      "Epoch:    3/10    Loss: 3.506166715145111    Least Loss: 3.5119603033065796\n",
      "      Model Saved ..\n",
      "\n",
      "Epoch:    3/10    Loss: 3.533324923276901    Least Loss: 3.506166715145111\n",
      "\n",
      "Epoch:    3/10    Loss: 3.509945168733597    Least Loss: 3.506166715145111\n",
      "\n",
      "Epoch:    3/10    Loss: 3.5592787635326384    Least Loss: 3.506166715145111\n",
      "\n",
      "Epoch:    3/10    Loss: 3.5287231187820436    Least Loss: 3.506166715145111\n",
      "\n",
      "Epoch:    3/10    Loss: 3.5405280957221983    Least Loss: 3.506166715145111\n",
      "\n",
      "Epoch:    3/10    Loss: 3.537943172931671    Least Loss: 3.506166715145111\n",
      "\n",
      "Epoch:    3/10    Loss: 3.5190791165828705    Least Loss: 3.506166715145111\n",
      "\n",
      "Epoch:    3/10    Loss: 3.598954805135727    Least Loss: 3.506166715145111\n",
      "\n",
      "Epoch:    3/10    Loss: 3.5730800931453706    Least Loss: 3.506166715145111\n",
      "\n",
      "Epoch:    3/10    Loss: 3.5607275390625    Least Loss: 3.506166715145111\n",
      "\n",
      "Epoch:    3/10    Loss: 3.5496367146968844    Least Loss: 3.506166715145111\n",
      "\n",
      "Epoch:    3/10    Loss: 3.607978477716446    Least Loss: 3.506166715145111\n",
      "\n",
      "Epoch:    3/10    Loss: 3.5470783219337463    Least Loss: 3.506166715145111\n",
      "\n",
      "Epoch:    3/10    Loss: 3.538587384223938    Least Loss: 3.506166715145111\n",
      "\n",
      "Epoch:    3/10    Loss: 3.5953024101257323    Least Loss: 3.506166715145111\n",
      "\n",
      "Epoch:    3/10    Loss: 3.564807331800461    Least Loss: 3.506166715145111\n",
      "\n",
      "Epoch:    3/10    Loss: 3.599835904598236    Least Loss: 3.506166715145111\n",
      "\n",
      "Epoch:    3/10    Loss: 3.5888403911590574    Least Loss: 3.506166715145111\n",
      "\n",
      "Epoch:    3/10    Loss: 3.5342233290672302    Least Loss: 3.506166715145111\n",
      "\n",
      "Epoch:    3/10    Loss: 3.561792780160904    Least Loss: 3.506166715145111\n",
      "\n",
      "Epoch:    3/10    Loss: 3.545615217924118    Least Loss: 3.506166715145111\n",
      "\n",
      "Epoch:    3/10    Loss: 3.530368337392807    Least Loss: 3.506166715145111\n",
      "\n",
      "Epoch:    3/10    Loss: 3.5595369901657103    Least Loss: 3.506166715145111\n",
      "\n",
      "Epoch:    3/10    Loss: 3.532668228149414    Least Loss: 3.506166715145111\n",
      "\n",
      "Epoch:    3/10    Loss: 3.5705685901641844    Least Loss: 3.506166715145111\n",
      "\n",
      "Epoch:    3/10    Loss: 3.5377182693481446    Least Loss: 3.506166715145111\n",
      "\n",
      "Epoch:    3/10    Loss: 3.541648404121399    Least Loss: 3.506166715145111\n",
      "\n",
      "Epoch:    3/10    Loss: 3.5701221289634706    Least Loss: 3.506166715145111\n",
      "\n",
      "Epoch:    3/10    Loss: 3.565265556812286    Least Loss: 3.506166715145111\n",
      "\n",
      "Epoch:    3/10    Loss: 3.596466164588928    Least Loss: 3.506166715145111\n",
      "\n",
      "Epoch:    3/10    Loss: 3.609129809141159    Least Loss: 3.506166715145111\n",
      "\n",
      "Epoch:    3/10    Loss: 3.5593793244361875    Least Loss: 3.506166715145111\n",
      "\n",
      "Epoch:    3/10    Loss: 3.5937370076179502    Least Loss: 3.506166715145111\n",
      "\n",
      "Epoch:    3/10    Loss: 3.5638782165050507    Least Loss: 3.506166715145111\n",
      "\n",
      "Epoch:    3/10    Loss: 3.5871686012744903    Least Loss: 3.506166715145111\n",
      "\n",
      "Epoch:    3/10    Loss: 3.528613832950592    Least Loss: 3.506166715145111\n",
      "\n",
      "Epoch:    3/10    Loss: 3.6199768462181092    Least Loss: 3.506166715145111\n",
      "\n",
      "Epoch:    3/10    Loss: 3.5139330961704256    Least Loss: 3.506166715145111\n",
      "\n",
      "Epoch:    3/10    Loss: 3.5699592661857604    Least Loss: 3.506166715145111\n",
      "\n",
      "Epoch:    3/10    Loss: 3.6223881788253784    Least Loss: 3.506166715145111\n",
      "\n",
      "Epoch:    3/10    Loss: 3.5578605086803434    Least Loss: 3.506166715145111\n",
      "\n",
      "Epoch:    3/10    Loss: 3.6161120047569275    Least Loss: 3.506166715145111\n",
      "\n",
      "Epoch:    3/10    Loss: 3.562010278463364    Least Loss: 3.506166715145111\n",
      "\n",
      "Epoch:    3/10    Loss: 3.60422917509079    Least Loss: 3.506166715145111\n",
      "\n",
      "Epoch:    3/10    Loss: 3.6081421885490417    Least Loss: 3.506166715145111\n",
      "\n",
      "Epoch:    3/10    Loss: 3.5996588332653046    Least Loss: 3.506166715145111\n",
      "\n",
      "Epoch:    3/10    Loss: 3.547365221500397    Least Loss: 3.506166715145111\n",
      "\n",
      "Epoch:    3/10    Loss: 3.5319536781311034    Least Loss: 3.506166715145111\n",
      "\n",
      "Epoch:    3/10    Loss: 3.5847898242473604    Least Loss: 3.506166715145111\n",
      "\n",
      "Epoch:    3/10    Loss: 3.5874623799324037    Least Loss: 3.506166715145111\n",
      "\n",
      "Epoch:    3/10    Loss: 3.5671034524440763    Least Loss: 3.506166715145111\n",
      "\n",
      "Epoch:    4/10    Loss: 3.5770691335541085    Least Loss: 3.506166715145111\n",
      "\n",
      "Epoch:    4/10    Loss: 3.4410605573654176    Least Loss: 3.506166715145111\n",
      "      Model Saved ..\n",
      "\n",
      "Epoch:    4/10    Loss: 3.5224872064590453    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    4/10    Loss: 3.5402423281669617    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    4/10    Loss: 3.5402475759983063    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    4/10    Loss: 3.4762738635540007    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    4/10    Loss: 3.551116013050079    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    4/10    Loss: 3.5652665116786957    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    4/10    Loss: 3.5117045810222627    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    4/10    Loss: 3.527807372093201    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    4/10    Loss: 3.4972384827136995    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    4/10    Loss: 3.5464846432209014    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    4/10    Loss: 3.559678327560425    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    4/10    Loss: 3.488421418428421    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    4/10    Loss: 3.53634393453598    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    4/10    Loss: 3.4879028820991516    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    4/10    Loss: 3.5306673736572267    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    4/10    Loss: 3.51238716673851    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    4/10    Loss: 3.5489415583610535    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    4/10    Loss: 3.5134430346488954    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    4/10    Loss: 3.5424118995666505    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    4/10    Loss: 3.53467994761467    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    4/10    Loss: 3.5128383753299715    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    4/10    Loss: 3.584744991064072    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    4/10    Loss: 3.5544424700737    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    4/10    Loss: 3.5304235806465147    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    4/10    Loss: 3.534398953437805    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    4/10    Loss: 3.558671916246414    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    4/10    Loss: 3.5370716710090635    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    4/10    Loss: 3.602886949777603    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    4/10    Loss: 3.520407620191574    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    4/10    Loss: 3.558697660446167    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    4/10    Loss: 3.53437873339653    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    4/10    Loss: 3.5992375395298004    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    4/10    Loss: 3.5619257543087004    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    4/10    Loss: 3.5787667260169984    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    4/10    Loss: 3.597831811904907    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    4/10    Loss: 3.5086260237693785    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    4/10    Loss: 3.5676159334182738    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    4/10    Loss: 3.5890811667442324    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    4/10    Loss: 3.5652306027412415    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    4/10    Loss: 3.546993562221527    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    4/10    Loss: 3.6227350883483886    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    4/10    Loss: 3.561388748884201    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    4/10    Loss: 3.487415704727173    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    4/10    Loss: 3.568864042520523    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    4/10    Loss: 3.5856909646987916    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    4/10    Loss: 3.5215926303863525    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    4/10    Loss: 3.5531085631847383    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    4/10    Loss: 3.566362087726593    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    4/10    Loss: 3.545373863220215    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    4/10    Loss: 3.560276810646057    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    4/10    Loss: 3.5681017317771913    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    4/10    Loss: 3.5849424889087675    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    4/10    Loss: 3.550612129688263    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    5/10    Loss: 3.5339005043732263    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    5/10    Loss: 3.5277715849876405    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    5/10    Loss: 3.45198512172699    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    5/10    Loss: 3.533918519496918    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    5/10    Loss: 3.496326998949051    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    5/10    Loss: 3.483369385957718    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    5/10    Loss: 3.553668541431427    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    5/10    Loss: 3.4654802622795104    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    5/10    Loss: 3.494924880504608    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    5/10    Loss: 3.5290225121974945    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    5/10    Loss: 3.528416513442993    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    5/10    Loss: 3.483955926179886    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    5/10    Loss: 3.551890950679779    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    5/10    Loss: 3.5135182938575746    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    5/10    Loss: 3.530935754776001    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    5/10    Loss: 3.500677553653717    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    5/10    Loss: 3.5164351410865784    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    5/10    Loss: 3.5079792358875275    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    5/10    Loss: 3.524979561328888    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    5/10    Loss: 3.54829651427269    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    5/10    Loss: 3.5225968952178954    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    5/10    Loss: 3.4986539855003356    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    5/10    Loss: 3.540181798458099    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    5/10    Loss: 3.519586226224899    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    5/10    Loss: 3.5357788944244386    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    5/10    Loss: 3.5290306468009947    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    5/10    Loss: 3.4775633676052093    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    5/10    Loss: 3.4987940611839297    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    5/10    Loss: 3.473076409816742    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    5/10    Loss: 3.578436859846115    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    5/10    Loss: 3.5139966645240786    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    5/10    Loss: 3.5381571135520935    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    5/10    Loss: 3.5159849739074707    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    5/10    Loss: 3.5437435054779054    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    5/10    Loss: 3.583259309053421    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    5/10    Loss: 3.5345637521743773    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    5/10    Loss: 3.5530074307918547    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    5/10    Loss: 3.5271866762638093    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    5/10    Loss: 3.5336279146671297    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    5/10    Loss: 3.555918851852417    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    5/10    Loss: 3.537420009613037    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    5/10    Loss: 3.5640380444526674    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    5/10    Loss: 3.525283444404602    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    5/10    Loss: 3.5989203853607177    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    5/10    Loss: 3.5535815501213075    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    5/10    Loss: 3.572531314373016    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    5/10    Loss: 3.6025704526901245    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    5/10    Loss: 3.520393020629883    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    5/10    Loss: 3.53186079287529    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    5/10    Loss: 3.552282398939133    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    5/10    Loss: 3.5007802431583404    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    5/10    Loss: 3.601124406337738    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    5/10    Loss: 3.55741992521286    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    5/10    Loss: 3.5325752131938932    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    5/10    Loss: 3.506318135499954    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    6/10    Loss: 3.5544697458143246    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    6/10    Loss: 3.485069530248642    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    6/10    Loss: 3.5020483906269075    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    6/10    Loss: 3.464824682235718    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    6/10    Loss: 3.494509062051773    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    6/10    Loss: 3.504078609228134    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    6/10    Loss: 3.487987713813782    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    6/10    Loss: 3.4867850775718687    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    6/10    Loss: 3.480474648475647    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    6/10    Loss: 3.4933880133628845    Least Loss: 3.4410605573654176\n",
      "\n",
      "Epoch:    6/10    Loss: 3.438812717676163    Least Loss: 3.4410605573654176\n",
      "      Model Saved ..\n",
      "\n",
      "Epoch:    6/10    Loss: 3.5140876870155333    Least Loss: 3.438812717676163\n",
      "\n",
      "Epoch:    6/10    Loss: 3.521087766647339    Least Loss: 3.438812717676163\n",
      "\n",
      "Epoch:    6/10    Loss: 3.4628104026317597    Least Loss: 3.438812717676163\n",
      "\n",
      "Epoch:    6/10    Loss: 3.5039864284992217    Least Loss: 3.438812717676163\n",
      "\n",
      "Epoch:    6/10    Loss: 3.4683160083293916    Least Loss: 3.438812717676163\n",
      "\n",
      "Epoch:    6/10    Loss: 3.4344438145160674    Least Loss: 3.438812717676163\n",
      "      Model Saved ..\n",
      "\n",
      "Epoch:    6/10    Loss: 3.5378392486572268    Least Loss: 3.4344438145160674\n",
      "\n",
      "Epoch:    6/10    Loss: 3.4656450028419497    Least Loss: 3.4344438145160674\n",
      "\n",
      "Epoch:    6/10    Loss: 3.497154019355774    Least Loss: 3.4344438145160674\n",
      "\n",
      "Epoch:    6/10    Loss: 3.570792356014252    Least Loss: 3.4344438145160674\n",
      "\n",
      "Epoch:    6/10    Loss: 3.5237292175292967    Least Loss: 3.4344438145160674\n",
      "\n",
      "Epoch:    6/10    Loss: 3.5038766334056852    Least Loss: 3.4344438145160674\n",
      "\n",
      "Epoch:    6/10    Loss: 3.544185515642166    Least Loss: 3.4344438145160674\n",
      "\n",
      "Epoch:    6/10    Loss: 3.4851952176094056    Least Loss: 3.4344438145160674\n",
      "\n",
      "Epoch:    6/10    Loss: 3.5725100395679474    Least Loss: 3.4344438145160674\n",
      "\n",
      "Epoch:    6/10    Loss: 3.4924728391170503    Least Loss: 3.4344438145160674\n",
      "\n",
      "Epoch:    6/10    Loss: 3.5376697459220887    Least Loss: 3.4344438145160674\n",
      "\n",
      "Epoch:    6/10    Loss: 3.511161107778549    Least Loss: 3.4344438145160674\n",
      "\n",
      "Epoch:    6/10    Loss: 3.5028354394435883    Least Loss: 3.4344438145160674\n",
      "\n",
      "Epoch:    6/10    Loss: 3.562781979084015    Least Loss: 3.4344438145160674\n",
      "\n",
      "Epoch:    6/10    Loss: 3.5410179793834686    Least Loss: 3.4344438145160674\n",
      "\n",
      "Epoch:    6/10    Loss: 3.505132526397705    Least Loss: 3.4344438145160674\n",
      "\n",
      "Epoch:    6/10    Loss: 3.5168278510570525    Least Loss: 3.4344438145160674\n",
      "\n",
      "Epoch:    6/10    Loss: 3.5280781497955322    Least Loss: 3.4344438145160674\n",
      "\n",
      "Epoch:    6/10    Loss: 3.5241083874702452    Least Loss: 3.4344438145160674\n",
      "\n",
      "Epoch:    6/10    Loss: 3.563938858270645    Least Loss: 3.4344438145160674\n",
      "\n",
      "Epoch:    6/10    Loss: 3.4866028428077698    Least Loss: 3.4344438145160674\n",
      "\n",
      "Epoch:    6/10    Loss: 3.5396367139816283    Least Loss: 3.4344438145160674\n",
      "\n",
      "Epoch:    6/10    Loss: 3.5701343829631806    Least Loss: 3.4344438145160674\n",
      "\n",
      "Epoch:    6/10    Loss: 3.4654978716373446    Least Loss: 3.4344438145160674\n",
      "\n",
      "Epoch:    6/10    Loss: 3.5383362383842467    Least Loss: 3.4344438145160674\n",
      "\n",
      "Epoch:    6/10    Loss: 3.5331212334632873    Least Loss: 3.4344438145160674\n",
      "\n",
      "Epoch:    6/10    Loss: 3.5131547544002535    Least Loss: 3.4344438145160674\n",
      "\n",
      "Epoch:    6/10    Loss: 3.539118339776993    Least Loss: 3.4344438145160674\n",
      "\n",
      "Epoch:    6/10    Loss: 3.53309881901741    Least Loss: 3.4344438145160674\n",
      "\n",
      "Epoch:    6/10    Loss: 3.5146227059364317    Least Loss: 3.4344438145160674\n",
      "\n",
      "Epoch:    6/10    Loss: 3.539351198196411    Least Loss: 3.4344438145160674\n",
      "\n",
      "Epoch:    6/10    Loss: 3.5656773092746734    Least Loss: 3.4344438145160674\n",
      "\n",
      "Epoch:    6/10    Loss: 3.487176607608795    Least Loss: 3.4344438145160674\n",
      "\n",
      "Epoch:    6/10    Loss: 3.5703120143413543    Least Loss: 3.4344438145160674\n",
      "\n",
      "Epoch:    6/10    Loss: 3.481360856294632    Least Loss: 3.4344438145160674\n",
      "\n",
      "Epoch:    6/10    Loss: 3.527071990966797    Least Loss: 3.4344438145160674\n",
      "\n",
      "Epoch:    6/10    Loss: 3.5733979914188385    Least Loss: 3.4344438145160674\n",
      "\n",
      "Epoch:    6/10    Loss: 3.5446441333293914    Least Loss: 3.4344438145160674\n",
      "\n",
      "Epoch:    7/10    Loss: 3.489873518014036    Least Loss: 3.4344438145160674\n",
      "\n",
      "Epoch:    7/10    Loss: 3.461177410364151    Least Loss: 3.4344438145160674\n",
      "\n",
      "Epoch:    7/10    Loss: 3.443796931266785    Least Loss: 3.4344438145160674\n",
      "\n",
      "Epoch:    7/10    Loss: 3.4156656889915467    Least Loss: 3.4344438145160674\n",
      "      Model Saved ..\n",
      "\n",
      "Epoch:    7/10    Loss: 3.4256236407756804    Least Loss: 3.4156656889915467\n",
      "\n",
      "Epoch:    7/10    Loss: 3.4844926142692567    Least Loss: 3.4156656889915467\n",
      "\n",
      "Epoch:    7/10    Loss: 3.470108323574066    Least Loss: 3.4156656889915467\n",
      "\n",
      "Epoch:    7/10    Loss: 3.5030970516204833    Least Loss: 3.4156656889915467\n",
      "\n",
      "Epoch:    7/10    Loss: 3.4777653844356538    Least Loss: 3.4156656889915467\n",
      "\n",
      "Epoch:    7/10    Loss: 3.4843328680992127    Least Loss: 3.4156656889915467\n",
      "\n",
      "Epoch:    7/10    Loss: 3.4817514271736143    Least Loss: 3.4156656889915467\n",
      "\n",
      "Epoch:    7/10    Loss: 3.4896602210998533    Least Loss: 3.4156656889915467\n",
      "\n",
      "Epoch:    7/10    Loss: 3.5144159264564516    Least Loss: 3.4156656889915467\n",
      "\n",
      "Epoch:    7/10    Loss: 3.4519074552059172    Least Loss: 3.4156656889915467\n",
      "\n",
      "Epoch:    7/10    Loss: 3.513624135732651    Least Loss: 3.4156656889915467\n",
      "\n",
      "Epoch:    7/10    Loss: 3.462906329870224    Least Loss: 3.4156656889915467\n",
      "\n",
      "Epoch:    7/10    Loss: 3.4651806750297545    Least Loss: 3.4156656889915467\n",
      "\n",
      "Epoch:    7/10    Loss: 3.3973730943202973    Least Loss: 3.4156656889915467\n",
      "      Model Saved ..\n",
      "\n",
      "Epoch:    7/10    Loss: 3.465637385368347    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    7/10    Loss: 3.519852788686752    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    7/10    Loss: 3.5687012631893156    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    7/10    Loss: 3.4236992797851564    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    7/10    Loss: 3.518514901638031    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    7/10    Loss: 3.523076921224594    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    7/10    Loss: 3.5477478852272033    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    7/10    Loss: 3.5143581006526947    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    7/10    Loss: 3.4724811131954194    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    7/10    Loss: 3.4893688311576843    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    7/10    Loss: 3.51321208024025    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    7/10    Loss: 3.4905410752296446    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    7/10    Loss: 3.529146080493927    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    7/10    Loss: 3.5677148394584655    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    7/10    Loss: 3.4758314633369447    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    7/10    Loss: 3.508963860988617    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    7/10    Loss: 3.5554787702560424    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    7/10    Loss: 3.500498503923416    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    7/10    Loss: 3.5280339269638064    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    7/10    Loss: 3.503065420627594    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    7/10    Loss: 3.4898555488586425    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    7/10    Loss: 3.528733535528183    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    7/10    Loss: 3.491513073205948    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    7/10    Loss: 3.5009854264259337    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    7/10    Loss: 3.5346799688339234    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    7/10    Loss: 3.543034663438797    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    7/10    Loss: 3.525792978286743    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    7/10    Loss: 3.479521873474121    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    7/10    Loss: 3.4850436618328096    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    7/10    Loss: 3.5294681277275086    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    7/10    Loss: 3.521077806711197    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    7/10    Loss: 3.558814779996872    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    7/10    Loss: 3.5551088979244234    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    7/10    Loss: 3.5451620087623597    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    7/10    Loss: 3.541896122932434    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    7/10    Loss: 3.526533987045288    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    7/10    Loss: 3.4802089335918427    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    8/10    Loss: 3.452210864835889    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    8/10    Loss: 3.445697741508484    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    8/10    Loss: 3.417845732927322    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    8/10    Loss: 3.5002993757724763    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    8/10    Loss: 3.44078645324707    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    8/10    Loss: 3.459700596809387    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    8/10    Loss: 3.434438844203949    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    8/10    Loss: 3.4506917004585267    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    8/10    Loss: 3.4576763191223145    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    8/10    Loss: 3.505924698829651    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    8/10    Loss: 3.4264630906581877    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    8/10    Loss: 3.4201455731391905    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    8/10    Loss: 3.497132617473602    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    8/10    Loss: 3.4202747974395753    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    8/10    Loss: 3.417930564403534    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    8/10    Loss: 3.4675110251903534    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    8/10    Loss: 3.484015122413635    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    8/10    Loss: 3.4951474826335907    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    8/10    Loss: 3.4923581879138945    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    8/10    Loss: 3.477316514968872    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    8/10    Loss: 3.496125638484955    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    8/10    Loss: 3.469596558332443    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    8/10    Loss: 3.4752650599479677    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    8/10    Loss: 3.505972103357315    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    8/10    Loss: 3.4728841009140017    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    8/10    Loss: 3.503584725141525    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    8/10    Loss: 3.5491660363674162    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    8/10    Loss: 3.5123241894245147    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    8/10    Loss: 3.508462322473526    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    8/10    Loss: 3.507642935037613    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    8/10    Loss: 3.52420361328125    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    8/10    Loss: 3.5079759039878846    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    8/10    Loss: 3.5065082268714907    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    8/10    Loss: 3.50790624666214    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    8/10    Loss: 3.506591113567352    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    8/10    Loss: 3.490301055431366    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    8/10    Loss: 3.4817600197792054    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    8/10    Loss: 3.5267418427467345    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    8/10    Loss: 3.5327723906040194    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    8/10    Loss: 3.4868728351593017    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    8/10    Loss: 3.48844495677948    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    8/10    Loss: 3.505865397453308    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    8/10    Loss: 3.490222210407257    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    8/10    Loss: 3.443374752283096    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    8/10    Loss: 3.514729313850403    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    8/10    Loss: 3.4706233060359954    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    8/10    Loss: 3.5231048974990844    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    8/10    Loss: 3.457089633703232    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    8/10    Loss: 3.5233042562007904    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    8/10    Loss: 3.5171332159042357    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    8/10    Loss: 3.522130220413208    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    8/10    Loss: 3.5436178872585296    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    8/10    Loss: 3.544079448699951    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    8/10    Loss: 3.4558290922641755    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    8/10    Loss: 3.4812220056056975    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    9/10    Loss: 3.46454762379419    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    9/10    Loss: 3.459192431688309    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    9/10    Loss: 3.45562797498703    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    9/10    Loss: 3.4627202775478363    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    9/10    Loss: 3.479355898618698    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    9/10    Loss: 3.456665529489517    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    9/10    Loss: 3.435787446975708    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    9/10    Loss: 3.4395635941028595    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    9/10    Loss: 3.4265628600120546    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    9/10    Loss: 3.445315786600113    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    9/10    Loss: 3.449591745376587    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    9/10    Loss: 3.431329679489136    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    9/10    Loss: 3.4609471387863158    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    9/10    Loss: 3.478617695093155    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    9/10    Loss: 3.4340940155982973    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    9/10    Loss: 3.4787960386276247    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    9/10    Loss: 3.487359349489212    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    9/10    Loss: 3.4739985315799715    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    9/10    Loss: 3.4448669493198394    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    9/10    Loss: 3.4144752950668336    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    9/10    Loss: 3.4911697080135347    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    9/10    Loss: 3.4673089685440064    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    9/10    Loss: 3.4514016811847688    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    9/10    Loss: 3.4939774954319    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    9/10    Loss: 3.4557964220046995    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    9/10    Loss: 3.472586352586746    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    9/10    Loss: 3.4341761803627016    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    9/10    Loss: 3.4591463854312896    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    9/10    Loss: 3.4642160985469816    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    9/10    Loss: 3.466697761297226    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    9/10    Loss: 3.506708133459091    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    9/10    Loss: 3.4661380012035368    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    9/10    Loss: 3.5457678780555724    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    9/10    Loss: 3.4696830894947053    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    9/10    Loss: 3.4532627997398375    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    9/10    Loss: 3.439594377040863    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    9/10    Loss: 3.4751407454013825    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    9/10    Loss: 3.473612519264221    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    9/10    Loss: 3.514314588546753    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    9/10    Loss: 3.497870090007782    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    9/10    Loss: 3.5347485473155977    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    9/10    Loss: 3.472355635881424    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    9/10    Loss: 3.5113447809219362    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    9/10    Loss: 3.4579271855354308    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    9/10    Loss: 3.472759958028793    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    9/10    Loss: 3.5122386841773987    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    9/10    Loss: 3.4511500811576843    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    9/10    Loss: 3.5057911210060118    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    9/10    Loss: 3.544075162887573    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    9/10    Loss: 3.4984520399570465    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    9/10    Loss: 3.454808255195618    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    9/10    Loss: 3.4441143248081207    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    9/10    Loss: 3.465561111211777    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    9/10    Loss: 3.502740966320038    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:    9/10    Loss: 3.541401798248291    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:   10/10    Loss: 3.455189805618034    Least Loss: 3.3973730943202973\n",
      "\n",
      "Epoch:   10/10    Loss: 3.3870738036632537    Least Loss: 3.3973730943202973\n",
      "      Model Saved ..\n",
      "\n",
      "Epoch:   10/10    Loss: 3.4388424644470215    Least Loss: 3.3870738036632537\n",
      "\n",
      "Epoch:   10/10    Loss: 3.4692698290348054    Least Loss: 3.3870738036632537\n",
      "\n",
      "Epoch:   10/10    Loss: 3.4061228637695313    Least Loss: 3.3870738036632537\n",
      "\n",
      "Epoch:   10/10    Loss: 3.4504435365200044    Least Loss: 3.3870738036632537\n",
      "\n",
      "Epoch:   10/10    Loss: 3.4346681473255156    Least Loss: 3.3870738036632537\n",
      "\n",
      "Epoch:   10/10    Loss: 3.3880331454277037    Least Loss: 3.3870738036632537\n",
      "\n",
      "Epoch:   10/10    Loss: 3.460366937160492    Least Loss: 3.3870738036632537\n",
      "\n",
      "Epoch:   10/10    Loss: 3.490931245326996    Least Loss: 3.3870738036632537\n",
      "\n",
      "Epoch:   10/10    Loss: 3.441785226345062    Least Loss: 3.3870738036632537\n",
      "\n",
      "Epoch:   10/10    Loss: 3.4728245668411253    Least Loss: 3.3870738036632537\n",
      "\n",
      "Epoch:   10/10    Loss: 3.3916675446033477    Least Loss: 3.3870738036632537\n",
      "\n",
      "Epoch:   10/10    Loss: 3.474867168188095    Least Loss: 3.3870738036632537\n",
      "\n",
      "Epoch:   10/10    Loss: 3.4205525600910187    Least Loss: 3.3870738036632537\n",
      "\n",
      "Epoch:   10/10    Loss: 3.4924845323562623    Least Loss: 3.3870738036632537\n",
      "\n",
      "Epoch:   10/10    Loss: 3.4493654651641847    Least Loss: 3.3870738036632537\n",
      "\n",
      "Epoch:   10/10    Loss: 3.4635130169391632    Least Loss: 3.3870738036632537\n",
      "\n",
      "Epoch:   10/10    Loss: 3.425794155836105    Least Loss: 3.3870738036632537\n",
      "\n",
      "Epoch:   10/10    Loss: 3.45633128285408    Least Loss: 3.3870738036632537\n",
      "\n",
      "Epoch:   10/10    Loss: 3.4954757680892943    Least Loss: 3.3870738036632537\n",
      "\n",
      "Epoch:   10/10    Loss: 3.493064388990402    Least Loss: 3.3870738036632537\n",
      "\n",
      "Epoch:   10/10    Loss: 3.4191514294147494    Least Loss: 3.3870738036632537\n",
      "\n",
      "Epoch:   10/10    Loss: 3.512712102651596    Least Loss: 3.3870738036632537\n",
      "\n",
      "Epoch:   10/10    Loss: 3.416870603561401    Least Loss: 3.3870738036632537\n",
      "\n",
      "Epoch:   10/10    Loss: 3.435716017484665    Least Loss: 3.3870738036632537\n",
      "\n",
      "Epoch:   10/10    Loss: 3.4383997094631193    Least Loss: 3.3870738036632537\n",
      "\n",
      "Epoch:   10/10    Loss: 3.4795307133197784    Least Loss: 3.3870738036632537\n",
      "\n",
      "Epoch:   10/10    Loss: 3.451702099084854    Least Loss: 3.3870738036632537\n",
      "\n",
      "Epoch:   10/10    Loss: 3.514045251607895    Least Loss: 3.3870738036632537\n",
      "\n",
      "Epoch:   10/10    Loss: 3.5309543075561525    Least Loss: 3.3870738036632537\n",
      "\n",
      "Epoch:   10/10    Loss: 3.4731289234161378    Least Loss: 3.3870738036632537\n",
      "\n",
      "Epoch:   10/10    Loss: 3.462015349149704    Least Loss: 3.3870738036632537\n",
      "\n",
      "Epoch:   10/10    Loss: 3.4328786122798918    Least Loss: 3.3870738036632537\n",
      "\n",
      "Epoch:   10/10    Loss: 3.5236552736759186    Least Loss: 3.3870738036632537\n",
      "\n",
      "Epoch:   10/10    Loss: 3.491761560201645    Least Loss: 3.3870738036632537\n",
      "\n",
      "Epoch:   10/10    Loss: 3.446737811088562    Least Loss: 3.3870738036632537\n",
      "\n",
      "Epoch:   10/10    Loss: 3.511191759347916    Least Loss: 3.3870738036632537\n",
      "\n",
      "Epoch:   10/10    Loss: 3.5214894788265227    Least Loss: 3.3870738036632537\n",
      "\n",
      "Epoch:   10/10    Loss: 3.497104592323303    Least Loss: 3.3870738036632537\n",
      "\n",
      "Epoch:   10/10    Loss: 3.4397977023124695    Least Loss: 3.3870738036632537\n",
      "\n",
      "Epoch:   10/10    Loss: 3.42835372543335    Least Loss: 3.3870738036632537\n",
      "\n",
      "Epoch:   10/10    Loss: 3.4610464379787444    Least Loss: 3.3870738036632537\n",
      "\n",
      "Epoch:   10/10    Loss: 3.442898117542267    Least Loss: 3.3870738036632537\n",
      "\n",
      "Epoch:   10/10    Loss: 3.410573344707489    Least Loss: 3.3870738036632537\n",
      "\n",
      "Epoch:   10/10    Loss: 3.4645969574451447    Least Loss: 3.3870738036632537\n",
      "\n",
      "Epoch:   10/10    Loss: 3.5050771872997286    Least Loss: 3.3870738036632537\n",
      "\n",
      "Epoch:   10/10    Loss: 3.4522928981781007    Least Loss: 3.3870738036632537\n",
      "\n",
      "Epoch:   10/10    Loss: 3.511421387910843    Least Loss: 3.3870738036632537\n",
      "\n",
      "Epoch:   10/10    Loss: 3.483435993909836    Least Loss: 3.3870738036632537\n",
      "\n",
      "Epoch:   10/10    Loss: 3.466851321935654    Least Loss: 3.3870738036632537\n",
      "\n",
      "Epoch:   10/10    Loss: 3.3979470381736756    Least Loss: 3.3870738036632537\n",
      "\n",
      "Epoch:   10/10    Loss: 3.4930526330471037    Least Loss: 3.3870738036632537\n",
      "\n",
      "Epoch:   10/10    Loss: 3.5207691578865052    Least Loss: 3.3870738036632537\n",
      "\n",
      "Epoch:   10/10    Loss: 3.4615197594165803    Least Loss: 3.3870738036632537\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.0001\n",
    "num_epochs = 10\n",
    "\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "with active_session():\n",
    "    trained_rnn = train_rnn(rnn, batch_size, optimizer, criterion, num_epochs, show_every_n_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: How did you decide on your model hyperparameters? \n",
    "For example, did you try different sequence_lengths and find that one size made the model converge faster? What about your hidden_dim and n_layers; how did you decide on those?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** \n",
    "I tried different sequence numbers ranging between 50 and 10, smaller sequence numbers converged better. Also, could reach smaller error with bigger hidden dimension. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Checkpoint\n",
    "\n",
    "After running the above training cell, your model will be saved by name, `trained_rnn`, and if you save your notebook progress, **you can pause here and come back to this code at another time**. You can resume your progress by running the next cell, which will load in our word:id dictionaries _and_ load in your saved model by name!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import torch\n",
    "import helper\n",
    "import problem_unittests as tests\n",
    "\n",
    "_, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()\n",
    "trained_rnn = helper.load_model('./save/trained_rnn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate TV Script\n",
    "With the network trained and saved, you'll use it to generate a new, \"fake\" Seinfeld TV script in this section.\n",
    "\n",
    "### Generate Text\n",
    "To generate the text, the network needs to start with a single word and repeat its predictions until it reaches a set length. You'll be using the `generate` function to do this. It takes a word id to start with, `prime_id`, and generates a set length of text, `predict_len`. Also note that it uses topk sampling to introduce some randomness in choosing the most likely next word, given an output set of word scores!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def generate(rnn, prime_id, int_to_vocab, token_dict, pad_value, predict_len=100):\n",
    "    \"\"\"\n",
    "    Generate text using the neural network\n",
    "    :param decoder: The PyTorch Module that holds the trained neural network\n",
    "    :param prime_id: The word id to start the first prediction\n",
    "    :param int_to_vocab: Dict of word id keys to word values\n",
    "    :param token_dict: Dict of puncuation tokens keys to puncuation values\n",
    "    :param pad_value: The value used to pad a sequence\n",
    "    :param predict_len: The length of text to generate\n",
    "    :return: The generated text\n",
    "    \"\"\"\n",
    "    rnn.eval()\n",
    "    \n",
    "    # create a sequence (batch_size=1) with the prime_id\n",
    "    current_seq = np.full((1, sequence_length), pad_value)\n",
    "    current_seq[-1][-1] = prime_id\n",
    "    predicted = [int_to_vocab[prime_id]]\n",
    "    \n",
    "    for _ in range(predict_len):\n",
    "        if train_on_gpu:\n",
    "            current_seq = torch.LongTensor(current_seq).cuda()\n",
    "        else:\n",
    "            current_seq = torch.LongTensor(current_seq)\n",
    "        \n",
    "        # initialize the hidden state\n",
    "        hidden = rnn.init_hidden(current_seq.size(0))\n",
    "        \n",
    "        # get the output of the rnn\n",
    "        output, _ = rnn(current_seq, hidden)\n",
    "        \n",
    "        # get the next word probabilities\n",
    "        p = F.softmax(output, dim=1).data\n",
    "        if(train_on_gpu):\n",
    "            p = p.cpu() # move to cpu\n",
    "         \n",
    "        # use top_k sampling to get the index of the next word\n",
    "        top_k = 5\n",
    "        p, top_i = p.topk(top_k)\n",
    "        top_i = top_i.numpy().squeeze()\n",
    "        \n",
    "        # select the likely next word index with some element of randomness\n",
    "        p = p.numpy().squeeze()\n",
    "        word_i = np.random.choice(top_i, p=p/p.sum())\n",
    "        \n",
    "        # retrieve that word from the dictionary\n",
    "        word = int_to_vocab[word_i]\n",
    "        predicted.append(word)     \n",
    "        \n",
    "        # the generated word becomes the next \"current sequence\" and the cycle can continue\n",
    "        current_seq = np.roll(current_seq, -1, 1)\n",
    "        current_seq[-1][-1] = word_i\n",
    "    \n",
    "    gen_sentences = ' '.join(predicted)\n",
    "    \n",
    "    # Replace punctuation tokens\n",
    "    for key, token in token_dict.items():\n",
    "        ending = ' ' if key in ['\\n', '(', '\"'] else ''\n",
    "        gen_sentences = gen_sentences.replace(' ' + token.lower(), key)\n",
    "    gen_sentences = gen_sentences.replace('\\n ', '\\n')\n",
    "    gen_sentences = gen_sentences.replace('( ', '(')\n",
    "    \n",
    "    # return all the sentences\n",
    "    return gen_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a New Script\n",
    "It's time to generate the text. Set `gen_length` to the length of TV script you want to generate and set `prime_word` to one of the following to start the prediction:\n",
    "- \"jerry\"\n",
    "- \"elaine\"\n",
    "- \"george\"\n",
    "- \"kramer\"\n",
    "\n",
    "You can set the prime word to _any word_ in our dictionary, but it's best to start with a name for generating a TV script. (You can also start with any other names you find in the original text file!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:43: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jerry: you want a little, you don't think you should do that?\n",
      "\n",
      "george: i think it's a little more thing in the car, and i saw you in the first time...\n",
      "\n",
      "jerry:(to jerry) hey.\n",
      "\n",
      "jerry: hey.\n",
      "\n",
      "elaine: hey, how you gonna get out of the coffee shop, i was going to do the only way to make up that i can see it.\n",
      "\n",
      "george: well, you know, i was just a little bit, you know, the one that i can do, but i got to see the doctor.\n",
      "\n",
      "jerry: oh, no no, no no, no, no, no, no, no.\n",
      "\n",
      "kramer: i know, i know.\n",
      "\n",
      "george: what about the big,?(elaine laughs)\n",
      "\n",
      "elaine:(to george) hey.\n",
      "\n",
      "jerry: hey, how do you know about the name?\n",
      "\n",
      "jerry: yeah.\n",
      "\n",
      "jerry:(smiling as she turns and walks away from the phone)\n",
      "\n",
      "kramer:(pointing out of his shirt).. you know...\n",
      "\n",
      "george: well, you know, i don't have to be able to do this.\n",
      "\n",
      "jerry: i can't get it out of my car.\n",
      "\n",
      "jerry: what? why?\n",
      "\n",
      "elaine:(to elaine) hey, what are you doing here?\n",
      "\n",
      "elaine: yeah, yeah, but you gotta have a good thing that i was in a little bit of a lot about this.\n",
      "\n",
      "kramer: yeah.\n",
      "\n",
      "jerry: so, i was thinking about the best.\n",
      "\n",
      "george: oh, my god...\n",
      "\n",
      "george: well, it's not a little more time.\n",
      "\n",
      "george:(to the phone) oh, you think you can get me to a man like that, i don't want to see it.\n",
      "\n",
      "george: well, you know, i just had to say something. i just don't see it!\n",
      "\n",
      "jerry: oh, no, no, no...\n"
     ]
    }
   ],
   "source": [
    "# run the cell multiple times to get different results!\n",
    "gen_length = 400 # modify the length to your preference\n",
    "prime_word = 'jerry' # name for starting the script\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "pad_word = helper.SPECIAL_WORDS['PADDING']\n",
    "generated_script = generate(trained_rnn, vocab_to_int[prime_word + ':'], int_to_vocab, token_dict, vocab_to_int[pad_word], gen_length)\n",
    "print(generated_script)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save your favorite scripts\n",
    "\n",
    "Once you have a script that you like (or find interesting), save it to a text file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save script to a text file\n",
    "f =  open(\"generated_script_1.txt\",\"w\")\n",
    "f.write(generated_script)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The TV Script is Not Perfect\n",
    "It's ok if the TV script doesn't make perfect sense. It should look like alternating lines of dialogue, here is one such example of a few generated lines.\n",
    "\n",
    "### Example generated script\n",
    "\n",
    ">jerry: what about me?\n",
    ">\n",
    ">jerry: i don't have to wait.\n",
    ">\n",
    ">kramer:(to the sales table)\n",
    ">\n",
    ">elaine:(to jerry) hey, look at this, i'm a good doctor.\n",
    ">\n",
    ">newman:(to elaine) you think i have no idea of this...\n",
    ">\n",
    ">elaine: oh, you better take the phone, and he was a little nervous.\n",
    ">\n",
    ">kramer:(to the phone) hey, hey, jerry, i don't want to be a little bit.(to kramer and jerry) you can't.\n",
    ">\n",
    ">jerry: oh, yeah. i don't even know, i know.\n",
    ">\n",
    ">jerry:(to the phone) oh, i know.\n",
    ">\n",
    ">kramer:(laughing) you know...(to jerry) you don't know.\n",
    "\n",
    "You can see that there are multiple characters that say (somewhat) complete sentences, but it doesn't have to be perfect! It takes quite a while to get good results, and often, you'll have to use a smaller vocabulary (and discard uncommon words), or get more data.  The Seinfeld dataset is about 3.4 MB, which is big enough for our purposes; for script generation you'll want more than 1 MB of text, generally. \n",
    "\n",
    "# Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook. Save the notebook file as \"dlnd_tv_script_generation.ipynb\" and save another copy as an HTML file by clicking \"File\" -> \"Download as..\"->\"html\". Include the \"helper.py\" and \"problem_unittests.py\" files in your submission. Once you download these files, compress them into one zip file for submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
